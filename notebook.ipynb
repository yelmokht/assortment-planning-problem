{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-424 - Combinatorial optimization - 2023-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arfani Salah-Eddine - salah-eddine.arfani@ulb.be - 000495528\n",
    "\n",
    "### El Mokhtari Younes - younes.el.mokhtari@ulb.be - 000479836\n",
    "\n",
    "### Jeq Ismail - ismail.jeq@ulb.be - 000494718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assortment Planning Problem (AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gurobipy in c:\\users\\jeqis\\appdata\\roaming\\python\\python312\\site-packages (11.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "medium_mu = pd.read_csv('data/medium-mu.csv', sep=';', header=None, dtype=float)\n",
    "medium_r = pd.read_csv('data/medium-r.csv', sep=';', header=None, dtype=float)\n",
    "small_mu = pd.read_csv('data/small-mu.csv', sep=';', header=None, dtype=float)\n",
    "small_r = pd.read_csv('data/small-r.csv', sep=';', header=None, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_parameters(n):\n",
    "    mu = np.random.uniform(0, 1, size = n + 1)\n",
    "    mu[0] = 0\n",
    "    r = np.random.uniform(0, 1, size = n + 1)\n",
    "    r[0] = 0\n",
    "    return mu, r\n",
    "\n",
    "def create_large_instances(m, n):\n",
    "    mu_file = 'data/large-mu.csv'\n",
    "    r_file = 'data/large-r.csv'\n",
    "    mu_data = []\n",
    "    r_data = []\n",
    "\n",
    "    for i in range(m):\n",
    "        mu, r = generate_parameters(n)\n",
    "        mu_data.append(mu)\n",
    "        r_data.append(r)\n",
    "        \n",
    "    # Takes too long to save the data\n",
    "    # mu_data = np.array(mu_data).T\n",
    "    # r_data = np.array(r_data).T\n",
    "\n",
    "    # f_mu_data = np.vectorize(lambda x: f\"{x:.18e}\")(mu_data)\n",
    "    # f_r_data = np.vectorize(lambda x: f\"{x:.18e}\")(r_data)\n",
    "\n",
    "    # mu_df = pd.DataFrame(f_mu_data)\n",
    "    # r_df = pd.DataFrame(f_r_data)\n",
    "\n",
    "    # mu_df.to_csv(mu_file, index=False, header=None, sep=';')\n",
    "    # r_df.to_csv(r_file, index=False, header=None, sep=';')\n",
    "\n",
    "    return mu_data, r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance\n",
    "def create_instances(data):\n",
    "    list_instances = []\n",
    "    for j in range(0, data.shape[1]):\n",
    "        list_instances.append(data.iloc[:,j].to_list())\n",
    "\n",
    "    return list_instances\n",
    "instances_small_mu = create_instances(small_mu)\n",
    "instances_small_r = create_instances(small_r)\n",
    "instances_medium_mu = create_instances(medium_mu)\n",
    "instances_medium_r = create_instances(medium_r)\n",
    "\n",
    "large_mu, large_r = create_large_instances(100, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we focus on the Assortment Planning Problem (AP), where a retailer wants to determine which products it has to propose to its customers in order to maximize its expected revenue. More precisely, consider a set of products $\\mathcal{I}=\\{1, \\ldots, n\\}$ that the retailer can propose to its customers, and selling product $i$ generates a net revenue of $r_{i}>0$ for him. We assume that the products are sorted in decreasing order of revenue, i.e. $r_{1}>r_{2}>\\cdots>r_{n}>0$, and that each product $i$ will be purchased according to a particular probability that depends on:\n",
    "\n",
    "- The mean utilities $\\left(\\mu_{j}\\right)_{j \\in \\mathcal{I}}$ for the customers when they buy product $j \\in \\mathcal{I}$,\n",
    "\n",
    "- The set of alternatives $\\mathcal{S}$ made available to the customers.\n",
    "\n",
    "These probabilities come from a discrete choice model called multinomial logit, and can be written as follows:\n",
    "\n",
    "$$\n",
    "P_{i}(\\mathcal{S})=\\frac{e^{\\mu_{i}}}{e^{\\mu_{0}}+\\sum_{j \\in \\mathcal{S}} e^{\\mu_{j}}}=\\frac{e^{\\mu_{i}}}{1+\\sum_{j \\in \\mathcal{S}} e^{\\mu_{j}}}, \\quad \\forall i \\in \\mathcal{I} \\cup\\{0\\}\n",
    "$$\n",
    "\n",
    "where $\\mu_{0}=0$ represents the utility - for the customers - of buying nothing. As we will see later on, it is convenient to assume that selling nothing does come with a revenue $r_{0} \\geqslant 0$ (that is, however, usually zero). With all of the above, the problem can be posed as the following combinatorial optimization problem:\n",
    "\n",
    "$$\n",
    "\\text { (AP) } \\max _{\\mathcal{S} \\subseteq \\mathcal{I}}\\left\\{r_{0} \\cdot P_{0}(\\mathcal{S})+\\sum_{i \\in \\mathcal{S}} r_{i} \\cdot P_{i}(\\mathcal{S})\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From combinatorial optimization to linear programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.1 Show that AP can be rewritten as the following integer programming problem:\n",
    "\n",
    "$$\n",
    "\\text { (AP-IP) } \\max _{x \\in\\{0,1\\}^{n}} \\frac{r_{0}+\\sum_{i=1}^{n} x_{i} r_{i} e^{\\mu_{i}}}{1+\\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}}\n",
    "$$\n",
    "\n",
    "#### Explain why this formulation is valid for the Assortment Planning Problem (variables, constraints, objective function), and why this formulation is nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert (AP) into an integer programming problem (AP-IP) by introducing binary decision variables $x_i$, where $x_i = 1$ if product $i$ is included in the assortment $\\mathcal{S}$, and $x_i = 0$ otherwise. We can rewrite the probabilities and revenues using $x_i$. For the no-purchase option, we have:\n",
    "\n",
    "$$\n",
    "P_0(\\mathcal{S}) = \\frac{1}{1 + \\sum_{i=1}^n x_i e^{\\mu_i}}\n",
    "$$\n",
    "\n",
    "For purchasing product $i$, we have:\n",
    "\n",
    "$$\n",
    "P_i(\\mathcal{S}) = \\frac{x_i e^{\\mu_i}}{1 + \\sum_{i=1}^n x_i e^{\\mu_i}}\n",
    "$$\n",
    "\n",
    "If we take the objective function of AP:\n",
    "\n",
    "$$\n",
    "r_0 \\cdot P_0(\\mathcal{S}) + \\sum_{i \\in \\mathcal{S}} r_i \\cdot P_i(\\mathcal{S})\n",
    "$$\n",
    "\n",
    "By replacing, we obtain:\n",
    "\n",
    "$$\n",
    "\\frac{r_0 + \\sum_{i=1}^n x_i r_i e^{\\mu_i}}{1 + \\sum_{i=1}^n x_i e^{\\mu_i}}\n",
    "$$\n",
    "\n",
    "This forms the objective function of (AP-IP):\n",
    "\n",
    "$$\n",
    "\\text { (AP-IP) } \\max _{x \\in\\{0,1\\}^{n}} \\frac{r_{0}+\\sum_{i=1}^{n} x_{i} r_{i} e^{\\mu_{i}}}{1+\\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}}\n",
    "$$\n",
    "\n",
    "This formulation is valid because the binary decision variables $x_i$ indicate whether each product is included in the assortment, the constraints ensure these variables remain binary and the objective function maximizes the revenue based on the probability and the selected products. Since the denominator of the objective function is a polynomial, the formulation is nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.2 Considering the following change of variables:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{0} & :=\\frac{1}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}} \\\\\n",
    "y_{i} & :=\\frac{x_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}}, \\quad \\forall i \\in \\mathcal{I}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### prove that AP-IP can be rewritten as the following problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { (AP-IPL) } \\max _{y, y_{0} \\geqslant 0} & r_{0} y_{0}+\\sum_{i=1}^{n} r_{i} y_{i} \\\\\n",
    "\\text { s.t. } & y_{0}+\\sum_{i=1}^{n} y_{i}=1 \\\\\n",
    "& y_{i} \\in\\left\\{0, y_{0} e^{\\mu_{i}}\\right\\},\n",
    "\\end{aligned} \\quad \\forall i \\in \\mathcal{I}\n",
    "$$\n",
    "\n",
    "#### where $y_{i} \\in\\left\\{0, y_{0} e^{\\mu_{i}}\\right\\}$ means that $y_{i}$ must take either 0 or $y_{0} e^{\\mu_{i}}$ as a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose the objective function of (AP-IP), we have:\n",
    "\n",
    "$$\n",
    "\\frac{r_0}{1 + \\sum_{j=1}^n x_j e^{\\mu_j}} + \\frac{\\sum_{j=1}^n x_j r_i e^{\\mu_j}}{1 + \\sum_{j=1}^n x_j e^{\\mu_j}}\n",
    "$$\n",
    "\n",
    "Using the change of variables, we immediately obtain the objective function of (AP-IPL):\n",
    "\n",
    "$$\n",
    "\\quad r_{0} y_{0} + \\sum_{i=1}^{n} r_{i} y_{i}\n",
    "$$\n",
    "\n",
    "Let's find the constraints now. Using the change of variables, we have:\n",
    "$$\n",
    "y_0 + \\sum_{i=1}^n y_i = \\frac{1}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}} + \\sum_{i=1}^{n} \\frac{x_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}} = \\frac{1 + \\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}}{1 + \\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}} = 1\n",
    "$$\n",
    "\n",
    "This gives us the first constraint:\n",
    "$$\n",
    "y_0 + \\sum_{i=1}^n y_i = 1\n",
    "$$\n",
    "\n",
    "Next, we consider the values $y_i$ can take. From the definition:\n",
    "$$\n",
    "y_i = \\frac{x_i e^{\\mu_i}}{1+\\sum_{j=1}^n x_j e^{\\mu_j}}\n",
    "$$\n",
    "\n",
    "When $x_i = 0$, we have:\n",
    "$$\n",
    "y_i = 0\n",
    "$$\n",
    "\n",
    "When $x_i = 1$, we have:\n",
    "$$\n",
    "y_i = \\frac{e^{\\mu_i}}{1+\\sum_{j=1}^n x_j e^{\\mu_j}} = y_0 e^{\\mu_i}\n",
    "$$\n",
    "\n",
    "This implies:\n",
    "$$\n",
    "y_i \\in \\{0, y_0 e^{\\mu_i}\\}, \\quad \\forall i \\in \\mathcal{I}\n",
    "$$\n",
    "\n",
    "Finally, the AP-IP problem can be rewritten as AP-IPL:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { (AP-IPL) } \\max _{y, y_{0} \\geqslant 0} & r_{0} y_{0}+\\sum_{i=1}^{n} r_{i} y_{i} \\\\\n",
    "\\text { s.t. } & y_{0}+\\sum_{i=1}^{n} y_{i}=1 \\\\\n",
    "& y_{i} \\in\\left\\{0, y_{0} e^{\\mu_{i}}\\right\\}, \\quad \\forall i \\in \\mathcal{I}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.3 Consider the continuous relaxation of AP-IPL:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "\\text{(AP-L)} \\quad \\max_{y, y_{0} \\geqslant 0} & r_{0} y_{0} + \\sum_{i=1}^{n} r_{i} y_{i} & \\\\\n",
    "\\text{s.t.} & y_{0} + \\sum_{i=1}^{n} y_{i} = 1 & \\\\\n",
    "& y_{i} \\leqslant y_{0} e^{\\mu_{i}} & \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### Prove that its linear programming dual is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\text{(AP-LD)} \\quad \\min_{\\pi \\geqslant 0, \\pi_{0} \\in \\mathbb{R}} & \\pi_{0} \\\\\n",
    "\\text{s.t.} & \\pi_{0} - \\sum_{i=1}^{n} \\pi_{i} e^{\\mu_{i}} \\geqslant r_{0} \\\\\n",
    "& \\pi_{0} + \\pi_{i} \\geqslant r_{i} \\quad \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### and explain the relationships between dual (resp. primal) variables and primal (resp. dual) constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the duality theorem to find (AP-LD). Let's reformulate the primal first, we have:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "\\max_{y, y_{0} \\geqslant 0} & r_{0} y_{0} + \\sum_{i=1}^{n} r_{i} y_{i} & \\\\\n",
    "\\text{s.t.} & y_{0} + \\sum_{i=1}^{n} y_{i} = 1 & \\\\\n",
    "& - y_{0} e^{\\mu_{i}} + y_{i} \\leqslant 0 & \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To formulate the dual problem, we associate dual variables with each constraint of the primal problem. Let's denote the dual variables as follows:\n",
    "\n",
    "- $\\pi_0$ for the first constraint $y_0 + \\sum_{i=1}^n y_i = 1$.\n",
    "- $\\pi_i$ for the second constraint $- y_{0} e^{\\mu_{i}} + y_{i} \\leqslant 0 $.\n",
    "\n",
    "The dual objective function is derived from the constant term in the constraints multiplied by the dual variables. So, we obtain:\n",
    "\n",
    "$$\n",
    "\\min_{\\pi_0, \\pi_i} \\quad \\pi_0\n",
    "$$\n",
    "\n",
    "\n",
    "The dual constraints can be derived from the coefficients of the primal constraints and the primal objective function, we have:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\pi_0 - \\sum_{i=1}^{n} \\pi_i e^{\\mu_i} \\geq r_0 \\\\\n",
    "\\pi_0 + \\pi_i \\geq r_i\n",
    "\\end{array} \\quad \\forall i \\in \\mathcal{I}\n",
    "$$\n",
    "\n",
    "Thus, the dual problem (AP-LD) is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\min_{\\pi \\geqslant 0, \\pi_{0} \\in \\mathbb{R}} & \\pi_{0} \\\\\n",
    "\\text{s.t.} & \\pi_{0} - \\sum_{i=1}^{n} \\pi_{i} e^{\\mu_{i}} \\geqslant r_{0} \\\\\n",
    "& \\pi_{0} + \\pi_{i} \\geqslant r_{i} \\quad \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "In the primal, we had:\n",
    "\n",
    "- $y_0$ which represents the probability of a customer of buying nothing given the set of products in the assortment.\n",
    "- $y_i$ which represents the probability of a customer of buying product $i$ given the set of products in the assortment.\n",
    "\n",
    "Now in the dual, we have:\n",
    "\n",
    "- $\\pi_0$ which is associated with the first primal constraint $y_0 + \\sum_{i=1}^n y_i = 1$. It measures the importance of ensuring that the total probability of all choices adds up to 100%. \n",
    "\n",
    "- $\\pi_i$ which is associated with the second primal constraint $- y_0 e^{\\mu_i} + y_i \\leq 0$ for each product $i \\in \\mathcal{I}$. It measures the importance of ensuring that the probability of choosing product $i$ fits within its attractiveness limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ideal formulation and a greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show that, with the help of AP-L, we can solve AP in polynomial time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 For any $k \\in \\mathcal{I}$, consider the following solution:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{0}^{k} & :=\\frac{1}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} \\\\\n",
    "y_{i}^{k} & :=\\left\\{\\begin{array}{ll}\n",
    "\\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} & \\text { if } i \\leqslant k \\\\\n",
    "0 & \\text { otherwise. }\n",
    "\\end{array} \\quad \\forall i \\in \\mathcal{I} .\\right.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.1 Show that $\\left(y^{k}, y_{0}^{k}\\right)$ is feasible for AP-L. What is its objective value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that $\\left(y^{k}, y_{0}^{k}\\right)$ is feasible for AP-L, we need to check that it satisfies the constraints of AP-L. For the constraint 1, we have:\n",
    "\n",
    "$$\n",
    "y_{0}^{k} + \\sum_{i=1}^{k} y_{i}^{k} = \\frac{1}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} + \\frac{\\sum_{i=1}^{k} e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} = \\frac{1 + \\sum_{i=1}^{k} e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} = 1\n",
    "$$\n",
    "\n",
    "Therefore, the first constraint is satisfied. For the second constraint, we have:\n",
    "\n",
    "For $ i \\leqslant k $:\n",
    "\n",
    "$$\n",
    "y_{i}^{k} = \\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} \\quad \\text{and} \\quad y_{0}^{k} e^{\\mu_{i}} = \\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "y_{i}^{k} = y_{0}^{k} e^{\\mu_{i}}\n",
    "$$\n",
    "\n",
    "For $ i > k $:\n",
    "\n",
    "$$\n",
    "y_{i}^{k} = 0 \\quad \\text{and} \\quad y_{0}^{k} e^{\\mu_{i}} = \\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} > 0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "y_{i}^{k} \\leqslant y_{0}^{k} e^{\\mu_{i}}\n",
    "$$\n",
    "\n",
    "Therefore, the second constraint is satisfied.\n",
    "\n",
    "Since both constraints are satisfied, $\\left(y^{k}, y_{0}^{k}\\right)$ is feasible for AP-L. Let's compute now the objective value for $\\left(y^{k}, y_{0}^{k}\\right)$:\n",
    "\n",
    "$$\n",
    "r_{0} y_{0}^{k} + \\sum_{i=1}^{n} r_{i} y_{i}^{k}\n",
    "$$\n",
    "\n",
    "We develop $y_0$ and $y_{0}^{k}$:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} + \\sum_{i=1}^{k} \\frac{r_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "After factoring, the objective value is:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.2 Using the change of variables in (1.2), prove that the associated $x^{k}$ is integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that the associated $x^{k}$ is integer, we use the change of variables defined in (1.2):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{0} & := \\frac{1}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}} \\\\\n",
    "y_{i} & := \\frac{x_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}}}, \\quad \\forall i \\in \\mathcal{I}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "y_{0}^{k} = \\frac{1}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "y_{i}^{k} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} & \\text{if } i \\leqslant k \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "We want to show that the associated $x^{k}$ is integer. From the change of variables, we have:\n",
    "\n",
    "$$\n",
    "x_{i} = \\frac{y_{i} (1+\\sum_{j=1}^{n} x_{j} e^{\\mu_{j}})}{e^{\\mu_{i}}}\n",
    "$$\n",
    "\n",
    "For $i \\leq k$:\n",
    "\n",
    "$$\n",
    "x_{i}^{k} = \\frac{y_{i}^{k} (1+\\sum_{j=1}^{k} e^{\\mu_{j}})}{e^{\\mu_{i}}}\n",
    "    = \\frac{\\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} (1+\\sum_{j=1}^{k} e^{\\mu_{j}})}{e^{\\mu_{i}}} \n",
    "    = \\frac{e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} \\cdot \\frac{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}{e^{\\mu_{i}}}\n",
    "    = \\frac{e^{\\mu_{i}}}{e^{\\mu_{i}}}\n",
    "    = 1\n",
    "$$\n",
    "\n",
    "For $i > k$:\n",
    "\n",
    "$$\n",
    "y_{i}^{k} = 0 \\implies x_{i}^{k} = \\frac{0 \\cdot (1+\\sum_{j=1}^{k} e^{\\mu_{j}})}{e^{\\mu_{i}}} = 0\n",
    "$$\n",
    "\n",
    "Therefore, the associated $x^{k}$ is integer, where:\n",
    "\n",
    "$$\n",
    "x_{i}^{k} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } i \\leq k \\\\\n",
    "0 & \\text{if } i > k\n",
    "\\end{array} \\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.3 Show that $\\left(y^{k}, y_{0}^{k}\\right)$ is a strictly better solution than $\\left(y^{k-1}, y_{0}^{k-1}\\right)$ if and only if:\n",
    "\n",
    "$$\n",
    "r_{k}>\\frac{r_{0}+\\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show this, we need to compare the objective values of $\\left(y^{k}, y_{0}^{k}\\right)$ and $\\left(y^{k-1}, y_{0}^{k-1}\\right)$.\n",
    "\n",
    "The objective value for $\\left(y^{k}, y_{0}^{k}\\right)$ is:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "The objective value for $\\left(y^{k-1}, y_{0}^{k-1}\\right)$ is:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "We need to show that:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}} > \\frac{r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "We can cross-multiply to compare the numerators directly:\n",
    "\n",
    "$$\n",
    "(r_{0} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i})(1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}) > (r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i})(1 + \\sum_{j=1}^{k} e^{\\mu_{j}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + r_{0} \\sum_{j=1}^{k-1} e^{\\mu_{j}} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i} \\sum_{j=1}^{k-1} e^{\\mu_{j}} > r_{0} + r_{0} \\sum_{j=1}^{k} e^{\\mu_{j}} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i} \\sum_{j=1}^{k} e^{\\mu_{j}}\n",
    "$$\n",
    "\n",
    "We cancel out the common terms:\n",
    "\n",
    "$$\n",
    "r_{0} \\sum_{j=1}^{k-1} e^{\\mu_{j}} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i} + \\sum_{i=1}^{k} e^{\\mu_{i}} r_{i} \\sum_{j=1}^{k-1} e^{\\mu_{j}} > r_{0} \\sum_{j=1}^{k} e^{\\mu_{j}} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i} \\sum_{j=1}^{k} e^{\\mu_{j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{\\mu_{k}} r_{k} + e^{\\mu_{k}} r_{k} \\sum_{j=1}^{k-1} e^{\\mu_{j}} > r_{0} e^{\\mu_{k}} + e^{\\mu_{k}} \\sum_{i=1}^{k-1} e^{\\mu_{j}} r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{k} + r_{k} \\sum_{j=1}^{k-1} e^{\\mu_{j}} > r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{j}} r_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "We can factor out $r_{k}$:\n",
    "\n",
    "$$\n",
    "r_{k} (1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}) > r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i}\n",
    "$$\n",
    "\n",
    "We divide both sides by $1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}$:\n",
    "\n",
    "$$\n",
    "r_{k} > \\frac{r_{0} + \\sum_{i=1}^{k-1} e^{\\mu_{i}} r_{i}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "Thus, $\\left(y^{k}, y_{0}^{k}\\right)$ is a strictly better solution than $\\left(y^{k-1}, y_{0}^{k}\\right)$ if and only if:\n",
    "\n",
    "$$\n",
    "r_{k} > \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.4 Prove that there is no $k \\in \\mathcal{I}$ such that\n",
    "\n",
    "$$\n",
    "r_{k} \\leqslant \\frac{r_{0}+\\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k-1} e^{\\mu_{j}}} \\quad \\text { and } \\quad r_{k+1}>\\frac{r_{0}+\\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "##### at the same time. What does the graph of the function $k \\rightarrow r^{\\top} y^{k}$ look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that both conditions cannot hold simultaneously, let's assume they do and show a contradiction.\n",
    "\n",
    "Suppose there exists a $k \\in \\mathcal{I}$ such that:\n",
    "\n",
    "1. $r_{k} \\leq \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}$\n",
    "2. $r_{k+1} > \\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}$\n",
    "\n",
    "Let's denote these values as follows:\n",
    "\n",
    "- $A = \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}$\n",
    "- $B = \\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}$\n",
    "\n",
    "So we have:\n",
    "\n",
    "$$\n",
    "r_{k} \\leq A \\quad \\text{and} \\quad r_{k+1} > B\n",
    "$$\n",
    "\n",
    "Let's compare $B$ and $A$. We can rewrite $B$ as:\n",
    "\n",
    "$$\n",
    "B = \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j} + e^{\\mu_{k}} r_{k}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}} + e^{\\mu_{k}}}\n",
    "$$\n",
    "\n",
    "By introducing $A$, we can rewrite $B$ in terms of $A$:\n",
    "\n",
    "$$\n",
    "B = \\frac{A \\cdot (1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}) + e^{\\mu_{k}} r_{k}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}} + e^{\\mu_{k}}}\n",
    "$$\n",
    "\n",
    "Since $r_k \\leq A$, we can replace $r_k$ with $A$ and we obtain that:\n",
    "\n",
    "$$\n",
    "B \\leq A\n",
    "$$\n",
    "\n",
    "This implies that $r_{k+1}$ cannot be greater than $B$ if $r_k \\leq A$ because it will mean that $r_{k+1} > r_k$, which is not possible based on our initial assumption of the problem.\n",
    "\n",
    "Therefore, it is impossible for both conditions to hold at the same time.\n",
    "\n",
    "The graph of the function $k \\rightarrow r^{\\top} y^{k}$ would show a non-increasing function because as more products are included in the assortment, the expected revenue for k products will tend to decrease or stay constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.5 Deduce that there is exactly one $k^{*} \\in \\mathcal{I}$ such that\n",
    "\n",
    "$$\n",
    "r_{1}>\\cdots>r_{k^{*}}>\\frac{r_{0}+\\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}}} \\quad \\text { and } \\quad \\frac{r_{0}+\\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k^{*}} e^{\\mu_{j}}} \\geqslant r_{k^{*}+1}>\\cdots>r_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.1.3, we showed that $y^{k}$ is a better solution than $y^{k-1}$ if and only if\n",
    "\n",
    "$$\n",
    "r_{k} > \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "From 2.1.4, we proved that there is no $k$ such that\n",
    "\n",
    "$$\n",
    "r_{k} \\leq \\frac{r_{0} + \\sum_{j=1}^{k-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k-1} e^{\\mu_{j}}} \\quad \\text{and} \\quad r_{k+1} > \\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "Based on these results, there is exactly one $k^{*}$ such that:\n",
    "\n",
    "$$\n",
    "r_{1} > \\cdots > r_{k^{*}} > \\frac{r_{0} + \\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}}} \\quad \\text{and} \\quad \\frac{r_{0} + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}}} \\geq r_{k^{*}+1} > \\cdots > r_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.1.6 Show that we cannot have $r_{k^{*}}<\\frac{r_{0}+\\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k *} e^{\\mu_{j}}}$, and deduce that we have\n",
    "\n",
    "$$\n",
    "r_{1}>\\cdots>r_{k^{*}} \\geqslant \\frac{r_{0}+\\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k^{*}} e^{\\mu_{j}}} \\geqslant r_{k^{*}+1}>\\cdots>r_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.1.5, we have:\n",
    "\n",
    "$$\n",
    "r_{1} > \\cdots > r_{k^{*}} > \\frac{r_{0} + \\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}-1} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "If $ r_{k^{*}} < \\frac{r_{0} + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}}} $, it would contradict our assumption since including product $ k^{*} $ should not diminish the value.\n",
    "\n",
    "Therefore, we must have:\n",
    "\n",
    "$$\n",
    "r_{k^{*}} \\geq \\frac{r_{0} + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "Hence, we deduce:\n",
    "\n",
    "$$\n",
    "r_{1} > \\cdots > r_{k^{*}} \\geq \\frac{r_{0} + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k^{*}} e^{\\mu_{j}}} \\geq r_{k^{*}+1} > \\cdots > r_{n}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 For any $k \\in \\mathcal{I}$, consider the following solution:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_{0}^{k} & :=\\frac{r_{0}+\\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} \\\\\n",
    "\\pi_{i}^{k} & :=\\left\\{\\begin{array}{ll}\n",
    "r_{i}-\\frac{r_{0}+\\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}} & \\text { if } i \\leqslant k \\\\\n",
    "0 & \\text { otherwise. }\n",
    "\\end{array}, \\quad \\forall i \\in \\mathcal{I} .\\right.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.1 What is its objective value? In which conditions is $\\left(\\pi^{k}, \\pi_{0}^{k}\\right)$ feasible for AP-LD? Is there such a $k$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective value of the dual problem (AP-LD) is $\\pi_{0}$. Thus, for this solution, the objective value is:\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} = \\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "\n",
    "To determine in which conditions $\\left(\\pi^{k}, \\pi_{0}^{k}\\right)$ is feasible for AP-LD, we need to check the constraints:\n",
    "\n",
    "1. $\\pi_{0} - \\sum_{i=1}^{n} \\pi_{i} e^{\\mu_{i}} \\geq r_{0}$\n",
    "2. $\\pi_{0} + \\pi_{i} \\geq r_{i} \\quad \\forall i \\in \\mathcal{I}$\n",
    "\n",
    "For the first constraint, we have for $i \\leq k$:\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} - \\sum_{i=1}^{k} \\left(r_{i} - \\pi_{0}^{k}\\right) e^{\\mu_{i}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} - \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}} + \\pi_{0}^{k} \\sum_{i=1}^{k} e^{\\mu_{i}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} \\left(1 + \\sum_{i=1}^{k} e^{\\mu_{i}}\\right) - \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left(\\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}}\\right) \\left(1 + \\sum_{i=1}^{k} e^{\\mu_{i}}\\right) - \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} - \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "This is always true as the terms cancel out. For $i > k$, we have:\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}} \\geq r_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} \\geq r_{0} \\left(1 + \\sum_{j=1}^{k} e^{\\mu_{j}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} \\geq r_{0} + r_{0} \\sum_{j=1}^{k} e^{\\mu_{j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} \\geq r_{0} \\sum_{j=1}^{k} e^{\\mu_{j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{k} e^{\\mu_{j}} (r_{j} - r_{0}) \\geq 0\n",
    "$$\n",
    "\n",
    "Since $r_{j} > r_{0}$ for all $j \\in \\mathcal{I}$, this inequality is always satisfied.\n",
    "\n",
    "For the second constraint, for $i \\leq k$, we have:\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} + \\pi_{i}^{k} \\geq r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} + \\left(r_{i} - \\pi_{0}^{k}\\right) \\geq r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{i} \\geq r_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This is always true.\n",
    "\n",
    "For $i > k$:\n",
    "\n",
    "$$\n",
    "\\pi_{0}^{k} \\geq r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j}}{1 + \\sum_{j=1}^{k} e^{\\mu_{j}}} \\geq r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} \\geq r_{i} \\left(1 + \\sum_{j=1}^{k} e^{\\mu_{j}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} r_{j} - r_{i} \\sum_{j=1}^{k} e^{\\mu_{j}} \\geq r_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{0} + \\sum_{j=1}^{k} e^{\\mu_{j}} (r_{j} - r_{i}) \\geq r_{i}\n",
    "$$\n",
    "\n",
    "Since $r_{j} > r_{i}$ for all $j \\leq k$, each term $(r_{j} - r_{i})$ is positive. Thus, $\\sum_{j=1}^{k} e^{\\mu_{j}} (r_{j} - r_{i})$ is positive or zero, making the left-hand side of the inequality greater than or equal to $r_{i}$.\n",
    "\n",
    "This implies that there exists a $k$ such that $\\pi_{0}^{k} \\geq r_{i}$ for all $i > k$, ensuring that $\\left(\\pi^{k}, \\pi_{0}^{k}\\right)$ is feasible for AP-LD for such $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.2 Given $k \\in \\mathcal{I}$, if $\\left(\\pi^{k}, \\pi_{0}^{k}\\right)$ is feasible for AP-LD, prove that $\\mathcal{S}:=\\{1, \\cdots, k\\}$ is optimal for AP. Is AP-L an ideal formulation for AP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2.1.1 and 2.2.1, we computed the objective value for (AP-L) and (AP-LD) respectively, and they have the same objective value for $k$:\n",
    "\n",
    "$$\n",
    "\\frac{r_{0} + \\sum_{i=1}^{k} r_{i} e^{\\mu_{i}}}{1+\\sum_{j=1}^{k} e^{\\mu_{j}}}\n",
    "$$\n",
    "\n",
    "And we proved that $\\left(\\pi^{k}, \\pi_{0}^{k}\\right)$ is feasible for AP-LD, it means that the dual problem (AP-LD) has an optimal solution with the objective value. Since the primal problem (AP-L) and the dual problem (AP-LD) have the same objective value, we can conclude that $\\mathcal{S} := \\{1, \\cdots, k\\}$ is optimal for AP.\n",
    "\n",
    "Since we found that $\\mathcal{S} := \\{1, \\cdots, k\\}$ is optimal for AP and it has the same objective value for (AP) and (AP-L), then we know that (AP-L) is an ideal formulation for AP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.3 Propose a polynomial time algorithm that solves AP. What is its worst-time complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the items are already sorted by decreasing revenue, we can use the following greedy algorithm:\n",
    "\n",
    "1. Initialize an empty set $\\mathcal{S}$ and set $k = 0$.\n",
    "2. For each product $i \\in \\mathcal{I}$:\n",
    "   - Compute the objective value if product $i$ is added to $\\mathcal{S}$:\n",
    "     $$\n",
    "     \\text{new value} = \\frac{r_{0} + \\sum_{j \\in \\mathcal{S} \\cup \\{i\\}} r_{j} e^{\\mu_{j}}}{1 + \\sum_{j \\in \\mathcal{S} \\cup \\{i\\}} e^{\\mu_{j}}}\n",
    "     $$\n",
    "   - Compute the objective value without adding product $i$:\n",
    "     $$\n",
    "     \\text{current value} = \\frac{r_{0} + \\sum_{j \\in \\mathcal{S}} r_{j} e^{\\mu_{j}}}{1 + \\sum_{j \\in \\mathcal{S}} e^{\\mu_{j}}}\n",
    "     $$\n",
    "   - If $\\text{new value} > \\text{current value}$, add product $i$ to $\\mathcal{S}$ and increment $k$ by 1.\n",
    "3. The set $\\mathcal{S}$ is the optimal assortment.\n",
    "\n",
    "Since the items are already sorted by decreasing revenue, no sorting is needed. For each product, computing the new objective value involves summing up to $k$ elements, which takes $O(k)$ time. In the worst case, $k = n$, so each iteration takes $O(n)$ time. With $n$ iterations, the total time complexity is $O(n^2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small instance n°1\n",
      "Small instance n°2\n",
      "Small instance n°3\n",
      "Small instance n°4\n",
      "Small instance n°5\n",
      "Small instance n°6\n",
      "Small instance n°7\n",
      "Small instance n°8\n",
      "Small instance n°9\n",
      "Small instance n°10\n",
      "Small instance n°11\n",
      "Small instance n°12\n",
      "Small instance n°13\n",
      "Small instance n°14\n",
      "Small instance n°15\n",
      "Small instance n°16\n",
      "Small instance n°17\n",
      "Small instance n°18\n",
      "Small instance n°19\n",
      "Small instance n°20\n",
      "Small instance n°21\n",
      "Small instance n°22\n",
      "Small instance n°23\n",
      "Small instance n°24\n",
      "Small instance n°25\n",
      "Small instance n°26\n",
      "Small instance n°27\n",
      "Small instance n°28\n",
      "Small instance n°29\n",
      "Small instance n°30\n",
      "Small instance n°31\n",
      "Small instance n°32\n",
      "Small instance n°33\n",
      "Small instance n°34\n",
      "Small instance n°35\n",
      "Small instance n°36\n",
      "Small instance n°37\n",
      "Small instance n°38\n",
      "Small instance n°39\n",
      "Small instance n°40\n",
      "Small instance n°41\n",
      "Small instance n°42\n",
      "Small instance n°43\n",
      "Small instance n°44\n",
      "Small instance n°45\n",
      "Small instance n°46\n",
      "Small instance n°47\n",
      "Small instance n°48\n",
      "Small instance n°49\n",
      "Small instance n°50\n",
      "Small instance n°51\n",
      "Small instance n°52\n",
      "Small instance n°53\n",
      "Small instance n°54\n",
      "Small instance n°55\n",
      "Small instance n°56\n",
      "Small instance n°57\n",
      "Small instance n°58\n",
      "Small instance n°59\n",
      "Small instance n°60\n",
      "Small instance n°61\n",
      "Small instance n°62\n",
      "Small instance n°63\n",
      "Small instance n°64\n",
      "Small instance n°65\n",
      "Small instance n°66\n",
      "Small instance n°67\n",
      "Small instance n°68\n",
      "Small instance n°69\n",
      "Small instance n°70\n",
      "Small instance n°71\n",
      "Small instance n°72\n",
      "Small instance n°73\n",
      "Small instance n°74\n",
      "Small instance n°75\n",
      "Small instance n°76\n",
      "Small instance n°77\n",
      "Small instance n°78\n",
      "Small instance n°79\n",
      "Small instance n°80\n",
      "Small instance n°81\n",
      "Small instance n°82\n",
      "Small instance n°83\n",
      "Small instance n°84\n",
      "Small instance n°85\n",
      "Small instance n°86\n",
      "Small instance n°87\n",
      "Small instance n°88\n",
      "Small instance n°89\n",
      "Small instance n°90\n",
      "Small instance n°91\n",
      "Small instance n°92\n",
      "Small instance n°93\n",
      "Small instance n°94\n",
      "Small instance n°95\n",
      "Small instance n°96\n",
      "Small instance n°97\n",
      "Small instance n°98\n",
      "Small instance n°99\n",
      "Small instance n°100\n",
      "{1: 0.0, 2: 0.0009999275207519531, 3: 0.0, 4: 0.0009999275207519531, 5: 0.0, 6: 0.0009999275207519531, 7: 0.0, 8: 0.0, 9: 0.0009999275207519531, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0009999275207519531, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0009999275207519531, 20: 0.0, 21: 0.0009999275207519531, 22: 0.0009999275207519531, 23: 0.0009999275207519531, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0010001659393310547, 28: 0.0010001659393310547, 29: 0.0, 30: 0.0, 31: 0.0009999275207519531, 32: 0.0, 33: 0.0010001659393310547, 34: 0.0, 35: 0.0010001659393310547, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0010001659393310547, 41: 0.0009999275207519531, 42: 0.0, 43: 0.0, 44: 0.0, 45: 0.0, 46: 0.0009999275207519531, 47: 0.0, 48: 0.0, 49: 0.0, 50: 0.0009999275207519531, 51: 0.0, 52: 0.0, 53: 0.0, 54: 0.0, 55: 0.0, 56: 0.0009999275207519531, 57: 0.0, 58: 0.0, 59: 0.0009999275207519531, 60: 0.0, 61: 0.0, 62: 0.0009999275207519531, 63: 0.0010001659393310547, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 0.0, 69: 0.0, 70: 0.0010001659393310547, 71: 0.0, 72: 0.0, 73: 0.0, 74: 0.0, 75: 0.0, 76: 0.0, 77: 0.0, 78: 0.0, 79: 0.0019998550415039062, 80: 0.0, 81: 0.0, 82: 0.0, 83: 0.0009999275207519531, 84: 0.0, 85: 0.0, 86: 0.0009999275207519531, 87: 0.0, 88: 0.0, 89: 0.0, 90: 0.0, 91: 0.0009999275207519531, 92: 0.0, 93: 0.0, 94: 0.0, 95: 0.0, 96: 0.0, 97: 0.0009999275207519531, 98: 0.0010001659393310547, 99: 0.0, 100: 0.0}\n",
      "{1: 0.6198641481852656, 2: 0.665778022248497, 3: 0.6339946629735003, 4: 0.5381948907172824, 5: 0.7980489581846234, 6: 0.7597182927148285, 7: 0.6923272582228941, 8: 0.7239474548707281, 9: 0.6082532031399059, 10: 0.6582189977831391, 11: 0.6979134866186782, 12: 0.6937936358040542, 13: 0.36158988455929675, 14: 0.643643829788082, 15: 0.6301191285387919, 16: 0.703063303616732, 17: 0.6529342845347634, 18: 0.5719687063811433, 19: 0.7736621239584691, 20: 0.8215100532919005, 21: 0.6039693690331019, 22: 0.5840008396636683, 23: 0.7002654382794304, 24: 0.5870423119027616, 25: 0.5639239449897495, 26: 0.6387336447048173, 27: 0.46665627380104824, 28: 0.5130263831246537, 29: 0.6861348432642971, 30: 0.47629352659105795, 31: 0.7034142605390751, 32: 0.6779967895254999, 33: 0.7268027539943671, 34: 0.4619450779463765, 35: 0.6811198277526705, 36: 0.7079042104369374, 37: 0.7061402195164916, 38: 0.6969502712354873, 39: 0.7688453773380569, 40: 0.6430490172675668, 41: 0.7597511806820524, 42: 0.27159245295649737, 43: 0.7123503086062363, 44: 0.555894647922259, 45: 0.6559557300705918, 46: 0.6744227970276669, 47: 0.55645877348104, 48: 0.6598082322462533, 49: 0.5287678504945665, 50: 0.6766075491946026, 51: 0.5686466144639117, 52: 0.5775836875834898, 53: 0.6185678851514161, 54: 0.6736624592411472, 55: 0.682132741835624, 56: 0.7001178868315409, 57: 0.6409622310065192, 58: 0.6285944561644837, 59: 0.6925459704070851, 60: 0.7085400424419916, 61: 0.4661506073154812, 62: 0.6502662431286481, 63: 0.6459262407647419, 64: 0.6334533612609957, 65: 0.5393222807815475, 66: 0.6897142768545804, 67: 0.6394986447218494, 68: 0.5099792376225571, 69: 0.6513052226167919, 70: 0.7803850302374272, 71: 0.7536008004241836, 72: 0.7304256520648239, 73: 0.3375609564419679, 74: 0.6590403091541527, 75: 0.5393826118402631, 76: 0.7104622278085393, 77: 0.7025420213449446, 78: 0.6672144402633495, 79: 0.5583537659723482, 80: 0.4819834465448475, 81: 0.37602789467798714, 82: 0.7774608632049526, 83: 0.6679041422016521, 84: 0.5662643690579006, 85: 0.6892995599013645, 86: 0.6353074222646182, 87: 0.6397066964662055, 88: 0.4396942650114388, 89: 0.6926367749002211, 90: 0.6406449025471112, 91: 0.7444865687539512, 92: 0.6208151365880067, 93: 0.517993851887793, 94: 0.6330622102055665, 95: 0.6822823466551922, 96: 0.5815540602792773, 97: 0.7655140156408617, 98: 0.6277470136105845, 99: 0.537729512568375, 100: 0.5818520131397866}\n",
      "{1: 3, 2: 1, 3: 2, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 4, 10: 2, 11: 2, 12: 2, 13: 3, 14: 10, 15: 3, 16: 4, 17: 5, 18: 2, 19: 4, 20: 6, 21: 3, 22: 4, 23: 10, 24: 5, 25: 3, 26: 4, 27: 3, 28: 3, 29: 10, 30: 3, 31: 3, 32: 3, 33: 5, 34: 3, 35: 2, 36: 4, 37: 2, 38: 4, 39: 5, 40: 3, 41: 3, 42: 2, 43: 10, 44: 4, 45: 4, 46: 3, 47: 5, 48: 3, 49: 2, 50: 10, 51: 3, 52: 4, 53: 3, 54: 4, 55: 3, 56: 10, 57: 3, 58: 3, 59: 4, 60: 10, 61: 2, 62: 3, 63: 2, 64: 2, 65: 3, 66: 10, 67: 3, 68: 3, 69: 4, 70: 4, 71: 4, 72: 3, 73: 4, 74: 10, 75: 3, 76: 2, 77: 6, 78: 3, 79: 3, 80: 10, 81: 4, 82: 10, 83: 3, 84: 3, 85: 4, 86: 4, 87: 5, 88: 2, 89: 2, 90: 3, 91: 3, 92: 3, 93: 3, 94: 3, 95: 2, 96: 3, 97: 3, 98: 3, 99: 2, 100: 4}\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import Model, GRB\n",
    "\n",
    "\n",
    "def solve_with_gurobi(k, r, mu):\n",
    "    model = gp.Model(\"AP-L\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "    # Variables\n",
    "    y0 = model.addVar(vtype=GRB.CONTINUOUS, name=\"y0\")\n",
    "    y = model.addVars(len(mu) - 1, vtype=GRB.CONTINUOUS, name=\"y\")\n",
    "\n",
    "    # Objective function\n",
    "    model.setObjective(r[0] * y0 + gp.quicksum(r[i + 1] * y[i] for i in range(k)), GRB.MAXIMIZE)\n",
    "\n",
    "    # Constraints\n",
    "    model.addConstr(y0 + gp.quicksum(y[i] for i in range(k)) == 1, \"probability_sum\")\n",
    "    for i in range(k):\n",
    "        model.addConstr(y[i] <= y0 * np.exp(mu[i + 1]), f\"probability{i}\")\n",
    "\n",
    "    # Optimize\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        y0_val = y0.X\n",
    "        y_vals = [y[i].X for i in range(k)]\n",
    "        return model\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "def greedy_algorithm_with_gurobi(r, mu):\n",
    "    n = len(r) - 1\n",
    "    best_value = -np.inf\n",
    "    best_k = 0\n",
    "    for k in range(1, n + 1):\n",
    "        model  = solve_with_gurobi(k, r, mu)\n",
    "        obj_val = model.ObjVal\n",
    "        if obj_val is not None and obj_val > best_value:\n",
    "            best_value = obj_val\n",
    "            best_k = k\n",
    "    return best_k, best_value, model.Runtime\n",
    "\n",
    "\n",
    "# Solve the AP_L for the small instances\n",
    "small_r = instances_small_r\n",
    "small_mu = instances_small_mu\n",
    "\n",
    "n = len(small_r[0]) - 1 # Number of products\n",
    "n_small_instances = len(small_r) # Number of instances\n",
    "\n",
    "small_instances_runtimes = {instance: None for instance in range(1,n_small_instances+1)}\n",
    "small_instances_k_values = {instance: None for instance in range(1,n_small_instances+1)}\n",
    "small_instances_optimal_values = {instance: None for instance in range(1,n_small_instances+1)}\n",
    "\n",
    "\n",
    "for i in range(n_small_instances):\n",
    "    print(f\"Small instance n°{i+1}\")\n",
    "    k, value, runtime = greedy_algorithm_with_gurobi(small_r[i], small_mu[i])\n",
    "    small_instances_runtimes[i+1]= runtime\n",
    "    small_instances_optimal_values[i+1] = value\n",
    "    small_instances_k_values[i+1] = k\n",
    "    \n",
    "print(small_instances_runtimes)\n",
    "print(small_instances_optimal_values)\n",
    "print(small_instances_k_values)\n",
    "\n",
    "# Solve the AP_L for the medium instances\n",
    "# medium_r = instances_medium_r\n",
    "# medium_mu = instances_medium_mu\n",
    "\n",
    "# n = len(medium_r[0]) - 1 # Number of products\n",
    "# n_medium_instances = len(medium_r) # Number of instances\n",
    "\n",
    "# medium_instances_runtimes = {instance: None for instance in range(1,n_medium_instances+1)}\n",
    "# medium_instances_optimal_values = {instance: None for instance in range(1,n_medium_instances+1)}\n",
    "# medium_instances_k_values = {instance: None for instance in range(1,n_medium_instances+1)}\n",
    "\n",
    "\n",
    "# for i in range(n_medium_instances):\n",
    "#     print(f\"medium instance n°{i+1}\")\n",
    "#     k,value,runtime = greedy_algorithm_with_gurobi(medium_r[i], medium_mu[i])\n",
    "#     medium_instances_runtimes[i+1]= runtime\n",
    "#     medium_instances_optimal_values[i+1] = value\n",
    "#     medium_instances_k_values[i+1] = k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A more practical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now suppose that the retailer can only offer up to $p$ products to its customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.1 Starting from the AP-L formulation, explain why the new model admits the following Mixed Integer Linear Program formulation (variables, constraints, objective function):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max _{y, y_{0} \\geqslant 0, z \\in\\{0,1\\}^{n}} & r_{0} y_{0}+\\sum_{i=1}^{n} r_{i} y_{i} \\\\\n",
    "\\text { s.t. } \\quad & y_{0}+\\sum_{i=1}^{n} y_{i}=1 \\\\\n",
    "& y_{i} \\leqslant y_{0} e^{\\mu_{i}}, \\\\\n",
    "& y \\leqslant z, \\quad \\sum_{i=1}^{n} z_{i} \\leqslant p\n",
    "\\end{aligned} \\quad \\forall i \\in \\mathcal{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MILP formulation, we can introduce binary variables $z_{i} \\in \\{0,1\\}$ to indicate whether product $i$ is included in the assortment. We will have to add two constraints:\n",
    "\n",
    "$$ y_{i} \\leq z_{i}  \\implies y \\leq z$$\n",
    "\n",
    "This ensures that $ y_{i} $ can only be positive if $ z_{i} = 1 $ and:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} z_{i} \\leq p $$\n",
    "\n",
    "This ensures that no more than $p$ products are included in the assortment. The objective function and the other constratins will not change from AP-L since the two constraints we added above will limit $y_i$ to 0 if we have more than p products. So our final MILP formulation is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max _{y_{0}, y \\geqslant 0, z \\in\\{0,1\\}^{n}} & r_{0} y_{0}+\\sum_{i=1}^{n} r_{i} y_{i} \\\\\n",
    "\\text { s.t. } \\quad & y_{0}+\\sum_{i=1}^{n} y_{i}=1 \\\\\n",
    "& y_{i} \\leqslant y_{0} e^{\\mu_{i}}, \\\\\n",
    "& y \\leqslant z, \\quad \\sum_{i=1}^{n} z_{i} \\leqslant p\n",
    "\\end{aligned} \\quad \\forall i \\in \\mathcal{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.2 Is the previous algorithm still working for the APC-MILP problem? Implement APC-MILP with one of the allowed solvers, and compare the results for $p \\in\\{1, n / 5, n / 2, n\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if the previous algorithm is still working for the APC-MILP problem, we need to understand the modifications introduced by the MILP formulation. The original algorithm was designed to solve the AP-L formulation, which involved continuous variables. The MILP formulation introduces binary variables, making the problem more complex and requiring a different solving approach.\n",
    "\n",
    "### Implementation\n",
    "Implementing the APC-MILP involves using a solver capable of handling mixed-integer linear programs. Commonly used solvers include Gurobi, CPLEX, and CBC. Below is a generic implementation outline using Python with a solver like Gurobi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small instance n°0\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°1\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°2\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°3\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°4\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°5\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°6\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°7\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°8\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°9\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°10\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°11\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°12\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°13\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°14\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°15\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°16\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°17\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°18\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°19\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°20\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°21\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°22\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°23\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°24\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°25\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°26\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°27\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°28\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°29\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°30\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°31\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°32\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°33\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°34\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°35\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°36\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°37\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°38\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°39\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°40\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°41\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°42\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°43\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°44\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°45\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°46\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°47\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°48\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°49\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°50\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°51\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°52\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°53\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°54\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°55\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°56\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°57\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°58\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°59\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°60\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°61\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°62\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°63\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°64\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°65\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°66\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°67\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°68\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°69\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°70\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°71\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°72\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°73\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°74\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°75\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°76\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°77\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°78\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°79\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°80\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°81\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°82\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°83\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°84\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°85\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°86\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°87\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°88\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°89\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°90\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°91\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°92\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°93\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°94\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°95\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°96\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°97\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°98\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "Small instance n°99\n",
      "p = 1\n",
      "p = 2\n",
      "p = 5\n",
      "p = 10\n",
      "{1: {1: 0.006000041961669922, 2: 0.006999969482421875, 5: 0.0009999275207519531, 10: 0.0019998550415039062}, 2: {1: 0.0009999275207519531, 2: 0.0010001659393310547, 5: 0.002000093460083008, 10: 0.002000093460083008}, 3: {1: 0.003000020980834961, 2: 0.0009999275207519531, 5: 0.0, 10: 0.002000093460083008}, 4: {1: 0.002000093460083008, 2: 0.004999876022338867, 5: 0.0010001659393310547, 10: 0.0010001659393310547}, 5: {1: 0.006000041961669922, 2: 0.004999876022338867, 5: 0.0010001659393310547, 10: 0.002000093460083008}, 6: {1: 0.00599980354309082, 2: 0.004999876022338867, 5: 0.0009999275207519531, 10: 0.0019998550415039062}, 7: {1: 0.003000020980834961, 2: 0.006000041961669922, 5: 0.0009999275207519531, 10: 0.0019998550415039062}, 8: {1: 0.003999948501586914, 2: 0.00800013542175293, 5: 0.0009999275207519531, 10: 0.0010001659393310547}, 9: {1: 0.005000114440917969, 2: 0.004999876022338867, 5: 0.0009999275207519531, 10: 0.0010001659393310547}, 10: {1: 0.003000020980834961, 2: 0.0009999275207519531, 5: 0.002000093460083008, 10: 0.0010001659393310547}, 11: {1: 0.004999876022338867, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 12: {1: 0.004999876022338867, 2: 0.002000093460083008, 5: 0.0010001659393310547, 10: 0.003000020980834961}, 13: {1: 0.003999948501586914, 2: 0.009000062942504883, 5: 0.002000093460083008, 10: 0.002000093460083008}, 14: {1: 0.006999969482421875, 2: 0.009999990463256836, 5: 0.0019998550415039062, 10: 0.002000093460083008}, 15: {1: 0.006000041961669922, 2: 0.006999969482421875, 5: 0.0019998550415039062, 10: 0.0010001659393310547}, 16: {1: 0.0070002079010009766, 2: 0.009000062942504883, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 17: {1: 0.006999969482421875, 2: 0.01100015640258789, 5: 0.002000093460083008, 10: 0.0009999275207519531}, 18: {1: 0.0019998550415039062, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 19: {1: 0.003999948501586914, 2: 0.009999990463256836, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 20: {1: 0.005000114440917969, 2: 0.009999990463256836, 5: 0.009999990463256836, 10: 0.0009999275207519531}, 21: {1: 0.005000114440917969, 2: 0.003999948501586914, 5: 0.0019998550415039062, 10: 0.0009999275207519531}, 22: {1: 0.006000041961669922, 2: 0.009000062942504883, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 23: {1: 0.006000041961669922, 2: 0.006000041961669922, 5: 0.0009999275207519531, 10: 0.0019998550415039062}, 24: {1: 0.010000228881835938, 2: 0.017999887466430664, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 25: {1: 0.006000041961669922, 2: 0.006000041961669922, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 26: {1: 0.005000114440917969, 2: 0.006000041961669922, 5: 0.002000093460083008, 10: 0.002000093460083008}, 27: {1: 0.004999876022338867, 2: 0.005000114440917969, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 28: {1: 0.004999876022338867, 2: 0.008999824523925781, 5: 0.002000093460083008, 10: 0.002000093460083008}, 29: {1: 0.011999845504760742, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.005000114440917969}, 30: {1: 0.004000186920166016, 2: 0.010999917984008789, 5: 0.003000020980834961, 10: 0.0009999275207519531}, 31: {1: 0.007999897003173828, 2: 0.009999990463256836, 5: 0.003000020980834961, 10: 0.003000020980834961}, 32: {1: 0.006000041961669922, 2: 0.009000062942504883, 5: 0.002000093460083008, 10: 0.003999948501586914}, 33: {1: 0.006999969482421875, 2: 0.014999866485595703, 5: 0.0019998550415039062, 10: 0.0009999275207519531}, 34: {1: 0.006000041961669922, 2: 0.004999876022338867, 5: 0.003000020980834961, 10: 0.0019998550415039062}, 35: {1: 0.003000020980834961, 2: 0.0009999275207519531, 5: 0.0010001659393310547, 10: 0.003000020980834961}, 36: {1: 0.003999948501586914, 2: 0.006999969482421875, 5: 0.0019998550415039062, 10: 0.003000020980834961}, 37: {1: 0.002000093460083008, 2: 0.002000093460083008, 5: 0.0010001659393310547, 10: 0.003000020980834961}, 38: {1: 0.009999990463256836, 2: 0.009000062942504883, 5: 0.002000093460083008, 10: 0.003000020980834961}, 39: {1: 0.006000041961669922, 2: 0.015000104904174805, 5: 0.002000093460083008, 10: 0.003000020980834961}, 40: {1: 0.00800013542175293, 2: 0.009000062942504883, 5: 0.002000093460083008, 10: 0.002000093460083008}, 41: {1: 0.009999990463256836, 2: 0.009999990463256836, 5: 0.005000114440917969, 10: 0.002000093460083008}, 42: {1: 0.004999876022338867, 2: 0.003000020980834961, 5: 0.002000093460083008, 10: 0.003000020980834961}, 43: {1: 0.00800013542175293, 2: 0.00800013542175293, 5: 0.0019998550415039062, 10: 0.003000020980834961}, 44: {1: 0.008999824523925781, 2: 0.013000011444091797, 5: 0.003000020980834961, 10: 0.002000093460083008}, 45: {1: 0.006000041961669922, 2: 0.011999845504760742, 5: 0.0029997825622558594, 10: 0.0009999275207519531}, 46: {1: 0.009999990463256836, 2: 0.00599980354309082, 5: 0.0019998550415039062, 10: 0.004000186920166016}, 47: {1: 0.013000011444091797, 2: 0.015000104904174805, 5: 0.003000020980834961, 10: 0.0019998550415039062}, 48: {1: 0.007999897003173828, 2: 0.006999969482421875, 5: 0.0019998550415039062, 10: 0.002000093460083008}, 49: {1: 0.009000062942504883, 2: 0.003000020980834961, 5: 0.002000093460083008, 10: 0.0009999275207519531}, 50: {1: 0.013000011444091797, 2: 0.01100015640258789, 5: 0.003000020980834961, 10: 0.0019998550415039062}, 51: {1: 0.006000041961669922, 2: 0.004999876022338867, 5: 0.002000093460083008, 10: 0.002000093460083008}, 52: {1: 0.0070002079010009766, 2: 0.006000041961669922, 5: 0.003000020980834961, 10: 0.0009999275207519531}, 53: {1: 0.009999990463256836, 2: 0.006000041961669922, 5: 0.002000093460083008, 10: 0.002000093460083008}, 54: {1: 0.009000062942504883, 2: 0.007999897003173828, 5: 0.004000186920166016, 10: 0.006000041961669922}, 55: {1: 0.026000022888183594, 2: 0.03500008583068848, 5: 0.005000114440917969, 10: 0.003999948501586914}, 56: {1: 0.006999969482421875, 2: 0.003000020980834961, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 57: {1: 0.006999969482421875, 2: 0.009000062942504883, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 58: {1: 0.005000114440917969, 2: 0.009999990463256836, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 59: {1: 0.009000062942504883, 2: 0.009999990463256836, 5: 0.002000093460083008, 10: 0.0009999275207519531}, 60: {1: 0.0010001659393310547, 2: 0.002000093460083008, 5: 0.003000020980834961, 10: 0.003000020980834961}, 61: {1: 0.003999948501586914, 2: 0.0009999275207519531, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 62: {1: 0.00599980354309082, 2: 0.006999969482421875, 5: 0.003000020980834961, 10: 0.003000020980834961}, 63: {1: 0.006000041961669922, 2: 0.003000020980834961, 5: 0.002000093460083008, 10: 0.0010001659393310547}, 64: {1: 0.007999897003173828, 2: 0.002000093460083008, 5: 0.0019998550415039062, 10: 0.0010001659393310547}, 65: {1: 0.013000011444091797, 2: 0.009999990463256836, 5: 0.002000093460083008, 10: 0.003999948501586914}, 66: {1: 0.00800013542175293, 2: 0.010999917984008789, 5: 0.002000093460083008, 10: 0.003999948501586914}, 67: {1: 0.004000186920166016, 2: 0.010999917984008789, 5: 0.0010001659393310547, 10: 0.0019998550415039062}, 68: {1: 0.009000062942504883, 2: 0.009999990463256836, 5: 0.0010001659393310547, 10: 0.0019998550415039062}, 69: {1: 0.006999969482421875, 2: 0.01399993896484375, 5: 0.0019998550415039062, 10: 0.0019998550415039062}, 70: {1: 0.006999969482421875, 2: 0.006999969482421875, 5: 0.0009999275207519531, 10: 0.003000020980834961}, 71: {1: 0.012000083923339844, 2: 0.015999794006347656, 5: 0.002000093460083008, 10: 0.002000093460083008}, 72: {1: 0.006000041961669922, 2: 0.008999824523925781, 5: 0.003000020980834961, 10: 0.003000020980834961}, 73: {1: 0.006999969482421875, 2: 0.012000083923339844, 5: 0.0019998550415039062, 10: 0.003000020980834961}, 74: {1: 0.006000041961669922, 2: 0.009999990463256836, 5: 0.0010001659393310547, 10: 0.0019998550415039062}, 75: {1: 0.007999897003173828, 2: 0.006000041961669922, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 76: {1: 0.003000020980834961, 2: 0.0009999275207519531, 5: 0.003000020980834961, 10: 0.003000020980834961}, 77: {1: 0.006999969482421875, 2: 0.017999887466430664, 5: 0.011999845504760742, 10: 0.002000093460083008}, 78: {1: 0.009000062942504883, 2: 0.006999969482421875, 5: 0.002000093460083008, 10: 0.003000020980834961}, 79: {1: 0.003000020980834961, 2: 0.007999897003173828, 5: 0.0019998550415039062, 10: 0.0009999275207519531}, 80: {1: 0.00800013542175293, 2: 0.010999917984008789, 5: 0.002000093460083008, 10: 0.0010001659393310547}, 81: {1: 0.006000041961669922, 2: 0.008999824523925781, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 82: {1: 0.003999948501586914, 2: 0.005000114440917969, 5: 0.002000093460083008, 10: 0.0010001659393310547}, 83: {1: 0.00800013542175293, 2: 0.009999990463256836, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 84: {1: 0.006999969482421875, 2: 0.005000114440917969, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 85: {1: 0.006000041961669922, 2: 0.012000083923339844, 5: 0.002000093460083008, 10: 0.0009999275207519531}, 86: {1: 0.006000041961669922, 2: 0.009000062942504883, 5: 0.0019998550415039062, 10: 0.003000020980834961}, 87: {1: 0.013000011444091797, 2: 0.03099989891052246, 5: 0.002000093460083008, 10: 0.0019998550415039062}, 88: {1: 0.009999990463256836, 2: 0.0019998550415039062, 5: 0.002000093460083008, 10: 0.003000020980834961}, 89: {1: 0.00599980354309082, 2: 0.0009999275207519531, 5: 0.003000020980834961, 10: 0.003000020980834961}, 90: {1: 0.004999876022338867, 2: 0.007999897003173828, 5: 0.0019998550415039062, 10: 0.002000093460083008}, 91: {1: 0.009999990463256836, 2: 0.005000114440917969, 5: 0.002000093460083008, 10: 0.003999948501586914}, 92: {1: 0.006000041961669922, 2: 0.009999990463256836, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 93: {1: 0.007999897003173828, 2: 0.006999969482421875, 5: 0.0009999275207519531, 10: 0.002000093460083008}, 94: {1: 0.006000041961669922, 2: 0.009000062942504883, 5: 0.0010001659393310547, 10: 0.002000093460083008}, 95: {1: 0.00800013542175293, 2: 0.002000093460083008, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 96: {1: 0.003999948501586914, 2: 0.003999948501586914, 5: 0.002000093460083008, 10: 0.0019998550415039062}, 97: {1: 0.006999969482421875, 2: 0.003999948501586914, 5: 0.002000093460083008, 10: 0.003000020980834961}, 98: {1: 0.0029997825622558594, 2: 0.004999876022338867, 5: 0.0009999275207519531, 10: 0.003000020980834961}, 99: {1: 0.004999876022338867, 2: 0.0019998550415039062, 5: 0.002000093460083008, 10: 0.0009999275207519531}, 100: {1: 0.008999824523925781, 2: 0.008999824523925781, 5: 0.0019998550415039062, 10: 0.0019998550415039062}}\n",
      "{1: {1: 0.5051598474010114, 2: 0.586121203824268, 5: 0.6198641481852656, 10: 0.6198641481852656}, 2: {1: 0.665778022248497, 2: 0.665778022248497, 5: 0.665778022248497, 10: 0.665778022248497}, 3: {1: 0.5860772381431256, 2: 0.6339946629735003, 5: 0.6339946629735003, 10: 0.6339946629735003}, 4: {1: 0.5002104764686907, 2: 0.5312540778442186, 5: 0.5381948907172824, 10: 0.5381948907172824}, 5: {1: 0.6368081973935169, 2: 0.7524656326428268, 5: 0.7980489581846234, 10: 0.7980489581846234}, 6: {1: 0.615711695902684, 2: 0.723571965671864, 5: 0.7597182927148285, 10: 0.7597182927148285}, 7: {1: 0.6167270302351567, 2: 0.6638133157126076, 5: 0.6923272582228941, 10: 0.6923272582228941}, 8: {1: 0.5881676473285752, 2: 0.692597066675673, 5: 0.7239474548707281, 10: 0.7239474548707281}, 9: {1: 0.48755645885707216, 2: 0.5998321405202052, 5: 0.608253203139906, 10: 0.608253203139906}, 10: {1: 0.6100996397043438, 2: 0.6582189977831391, 5: 0.6582189977831391, 10: 0.6582189977831391}, 11: {1: 0.6238934175515555, 2: 0.6979134866186782, 5: 0.6979134866186782, 10: 0.6979134866186782}, 12: {1: 0.6128781941669699, 2: 0.6937936358040542, 5: 0.6937936358040542, 10: 0.6937936358040542}, 13: {1: 0.29917068520107803, 2: 0.34438283717439266, 5: 0.36158988455929675, 10: 0.36158988455929675}, 14: {1: 0.5126429548780473, 2: 0.5967025916444167, 5: 0.6436438297880819, 10: 0.643643829788082}, 15: {1: 0.5171097916937513, 2: 0.6271872842888628, 5: 0.630119128538792, 10: 0.630119128538792}, 16: {1: 0.5634031536123149, 2: 0.6541185366621931, 5: 0.703063303616732, 10: 0.703063303616732}, 17: {1: 0.5096404334498239, 2: 0.5983650567024612, 5: 0.6529342845347634, 10: 0.6529342845347634}, 18: {1: 0.5192165501049865, 2: 0.5719687063811433, 5: 0.5719687063811434, 10: 0.5719687063811434}, 19: {1: 0.6429737470941655, 2: 0.7233616937703156, 5: 0.7736621239584691, 10: 0.7736621239584691}, 20: {1: 0.6886713967992117, 2: 0.7687711664198243, 5: 0.8186146834044977, 10: 0.8215100532919005}, 21: {1: 0.4999667916933303, 2: 0.6036603403443047, 5: 0.6039693690331019, 10: 0.6039693690331019}, 22: {1: 0.46821563862252275, 2: 0.544485953157777, 5: 0.5840008396636683, 10: 0.5840008396636683}, 23: {1: 0.568434756965498, 2: 0.6612640410199914, 5: 0.7002654382794303, 10: 0.7002654382794303}, 24: {1: 0.4611381704771727, 2: 0.5457819777950649, 5: 0.5870423119027616, 10: 0.5870423119027617}, 25: {1: 0.4624609715593937, 2: 0.5374709766208696, 5: 0.5639239449897495, 10: 0.5639239449897495}, 26: {1: 0.5504207042118355, 2: 0.625269768748903, 5: 0.6387336447048173, 10: 0.6387336447048173}, 27: {1: 0.3713707255124445, 2: 0.4568470345607166, 5: 0.46665627380104824, 10: 0.46665627380104824}, 28: {1: 0.3985818607466571, 2: 0.4756202366592778, 5: 0.5130263831246537, 10: 0.5130263831246537}, 29: {1: 0.5722708557427066, 2: 0.686134843264297, 5: 0.686134843264297, 10: 0.686134843264297}, 30: {1: 0.4586099162005257, 2: 0.4702525404785298, 5: 0.476293526591058, 10: 0.476293526591058}, 31: {1: 0.5852970225283178, 2: 0.6696113107292608, 5: 0.7034142605390751, 10: 0.7034142605390751}, 32: {1: 0.5875327959066817, 2: 0.6442670662775203, 5: 0.6779967895254999, 10: 0.6779967895254999}, 33: {1: 0.5560818730666934, 2: 0.6628087238043924, 5: 0.7268027539943671, 10: 0.7268027539943671}, 34: {1: 0.44249727979186615, 2: 0.4560447146098478, 5: 0.4619450779463765, 10: 0.4619450779463765}, 35: {1: 0.6298634415132314, 2: 0.6811198277526705, 5: 0.6811198277526705, 10: 0.6811198277526705}, 36: {1: 0.6229605602259193, 2: 0.6749750933601104, 5: 0.7079042104369374, 10: 0.7079042104369375}, 37: {1: 0.6975114820498299, 2: 0.7061402195164916, 5: 0.7061402195164916, 10: 0.7061402195164916}, 38: {1: 0.5226486324945635, 2: 0.638376686859125, 5: 0.6969502712354873, 10: 0.6969502712354873}, 39: {1: 0.6579081217391233, 2: 0.7241818414716301, 5: 0.7688453773380569, 10: 0.7688453773380569}, 40: {1: 0.5523203814098541, 2: 0.6215181021644299, 5: 0.6430490172675668, 10: 0.6430490172675668}, 41: {1: 0.5858678378893509, 2: 0.7209633299236818, 5: 0.7597511806820524, 10: 0.7597511806820524}, 42: {1: 0.2557855016505, 2: 0.27159245295649737, 5: 0.27159245295649737, 10: 0.27159245295649737}, 43: {1: 0.5815699122170171, 2: 0.6813464540830511, 5: 0.7123503086062363, 10: 0.7123503086062363}, 44: {1: 0.43204487703987104, 2: 0.5118536005808151, 5: 0.555894647922259, 10: 0.5558946479222591}, 45: {1: 0.49819406807504657, 2: 0.6055916943057549, 5: 0.6559557300705919, 10: 0.6559557300705918}, 46: {1: 0.5939364297896605, 2: 0.6675388551639945, 5: 0.6744227970276669, 10: 0.6744227970276669}, 47: {1: 0.45368943715180643, 2: 0.5300800964032737, 5: 0.55645877348104, 10: 0.55645877348104}, 48: {1: 0.5701853937774584, 2: 0.6389428192889826, 5: 0.6598082322462533, 10: 0.6598082322462533}, 49: {1: 0.4664899041719747, 2: 0.5287678504945665, 5: 0.5287678504945665, 10: 0.5287678504945665}, 50: {1: 0.5678500088457547, 2: 0.6393593511351159, 5: 0.6766075491946025, 10: 0.6766075491946025}, 51: {1: 0.4944316199992456, 2: 0.5619066852064657, 5: 0.5686466144639117, 10: 0.5686466144639117}, 52: {1: 0.46726307028245945, 2: 0.568915087030915, 5: 0.5775836875834898, 10: 0.5775836875834897}, 53: {1: 0.5400719484934353, 2: 0.6122944267069377, 5: 0.6185678851514161, 10: 0.6185678851514161}, 54: {1: 0.5811325934342193, 2: 0.654905515360739, 5: 0.6736624592411472, 10: 0.6736624592411473}, 55: {1: 0.5510048679979019, 2: 0.6715020763756667, 5: 0.682132741835624, 10: 0.682132741835624}, 56: {1: 0.611687679359684, 2: 0.7001178868315407, 5: 0.7001178868315407, 10: 0.7001178868315407}, 57: {1: 0.5353695634733238, 2: 0.6199772343703109, 5: 0.6409622310065192, 10: 0.6409622310065192}, 58: {1: 0.48018553752733756, 2: 0.5868761445687096, 5: 0.6285944561644837, 10: 0.6285944561644837}, 59: {1: 0.5530266113917623, 2: 0.6422543692702967, 5: 0.6925459704070851, 10: 0.6925459704070852}, 60: {1: 0.7085400424419916, 2: 0.7085400424419916, 5: 0.7085400424419916, 10: 0.7085400424419916}, 61: {1: 0.41794717770203077, 2: 0.4661506073154812, 5: 0.4661506073154812, 10: 0.4661506073154812}, 62: {1: 0.5741828440306019, 2: 0.6343283011524296, 5: 0.6502662431286481, 10: 0.6502662431286481}, 63: {1: 0.5540511652208987, 2: 0.6459262407647419, 5: 0.6459262407647419, 10: 0.6459262407647419}, 64: {1: 0.5976637535928131, 2: 0.6334533612609957, 5: 0.6334533612609957, 10: 0.6334533612609957}, 65: {1: 0.45789336825332805, 2: 0.5336655044010232, 5: 0.5393222807815475, 10: 0.5393222807815475}, 66: {1: 0.5867729850267434, 2: 0.6519536826197921, 5: 0.6897142768545804, 10: 0.6897142768545804}, 67: {1: 0.5788818527745545, 2: 0.6234372346794723, 5: 0.6394986447218494, 10: 0.6394986447218494}, 68: {1: 0.4059153834647778, 2: 0.4750563518250685, 5: 0.5099792376225573, 10: 0.5099792376225573}, 69: {1: 0.5784712970971391, 2: 0.6363622651634521, 5: 0.6513052226167919, 10: 0.6513052226167919}, 70: {1: 0.6277888432299321, 2: 0.7292346275067645, 5: 0.7803850302374272, 10: 0.7803850302374272}, 71: {1: 0.5545294817735436, 2: 0.6813522384743084, 5: 0.7536008004241836, 10: 0.7536008004241836}, 72: {1: 0.598888485471782, 2: 0.6932159973608956, 5: 0.7304256520648239, 10: 0.7304256520648239}, 73: {1: 0.29130576254052915, 2: 0.327096085476337, 5: 0.3375609564419679, 10: 0.3375609564419679}, 74: {1: 0.49420792821667153, 2: 0.6311047406603278, 5: 0.6590403091541527, 10: 0.6590403091541527}, 75: {1: 0.44255737013951213, 2: 0.5310041548478399, 5: 0.5393826118402631, 10: 0.5393826118402631}, 76: {1: 0.6640981787402246, 2: 0.7104622278085393, 5: 0.7104622278085393, 10: 0.7104622278085393}, 77: {1: 0.49609331349347774, 2: 0.6038282593888727, 5: 0.6943404787254448, 10: 0.7025420213449447}, 78: {1: 0.5782745179129638, 2: 0.6483736134442755, 5: 0.6672144402633495, 10: 0.6672144402633495}, 79: {1: 0.5114958477900888, 2: 0.5531464480387406, 5: 0.5583537659723482, 10: 0.5583537659723482}, 80: {1: 0.38556737182120465, 2: 0.44625874976998203, 5: 0.4819834465448474, 10: 0.4819834465448475}, 81: {1: 0.2758770905098237, 2: 0.359722136487049, 5: 0.37602789467798714, 10: 0.3760278946779871}, 82: {1: 0.6494214350907989, 2: 0.7446133777014565, 5: 0.7774608632049526, 10: 0.7774608632049526}, 83: {1: 0.5592751604916226, 2: 0.6398573044309988, 5: 0.6679041422016522, 10: 0.6679041422016522}, 84: {1: 0.44079176097952955, 2: 0.5463461184532776, 5: 0.5662643690579006, 10: 0.5662643690579006}, 85: {1: 0.562710006231181, 2: 0.6529631718190354, 5: 0.6892995599013645, 10: 0.6892995599013645}, 86: {1: 0.5496006643472087, 2: 0.6127215771519083, 5: 0.6353074222646182, 10: 0.6353074222646182}, 87: {1: 0.4507122348309681, 2: 0.568082053199975, 5: 0.6397066964662055, 10: 0.6397066964662055}, 88: {1: 0.3426531258655089, 2: 0.4396942650114388, 5: 0.4396942650114388, 10: 0.4396942650114388}, 89: {1: 0.6048218097395497, 2: 0.6926367749002211, 5: 0.6926367749002211, 10: 0.6926367749002211}, 90: {1: 0.5382793344973497, 2: 0.6361070465737029, 5: 0.6406449025471112, 10: 0.6406449025471112}, 91: {1: 0.6354089344759333, 2: 0.7351735860448615, 5: 0.7444865687539512, 10: 0.7444865687539512}, 92: {1: 0.4765568952604912, 2: 0.5813220816161206, 5: 0.6208151365880067, 10: 0.6208151365880067}, 93: {1: 0.4463184147105359, 2: 0.4987806381219162, 5: 0.517993851887793, 10: 0.517993851887793}, 94: {1: 0.5388981102219531, 2: 0.6255964830922344, 5: 0.6330622102055665, 10: 0.6330622102055665}, 95: {1: 0.6118731243199949, 2: 0.6822823466551922, 5: 0.6822823466551922, 10: 0.6822823466551922}, 96: {1: 0.5324543076401836, 2: 0.5795888686712516, 5: 0.5815540602792773, 10: 0.5815540602792773}, 97: {1: 0.6545519985628638, 2: 0.7519861856394785, 5: 0.7655140156408617, 10: 0.7655140156408617}, 98: {1: 0.5903685608845293, 2: 0.6256136663780942, 5: 0.6277470136105845, 10: 0.6277470136105845}, 99: {1: 0.4564165293113215, 2: 0.537729512568375, 5: 0.537729512568375, 10: 0.537729512568375}, 100: {1: 0.45089873809672754, 2: 0.5373127008662751, 5: 0.5818520131397867, 10: 0.5818520131397866}}\n",
      "Medium instance n°1\n",
      "p = 1\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import Model, GRB\n",
    "\n",
    "def solve_apc_milp(r, mu, p, n):\n",
    "    model = Model(\"APC-MILP\")\n",
    "    \n",
    "    model.setParam('OutputFlag', 0)\n",
    "    # Variables\n",
    "    y0 = model.addVar(vtype=GRB.CONTINUOUS, name=\"y0\")\n",
    "    y = model.addVars(n, vtype=GRB.CONTINUOUS, name=\"y\")\n",
    "    z = model.addVars(n, vtype=GRB.BINARY, name=\"z\")\n",
    "\n",
    "    # Objective function\n",
    "    model.setObjective(r[0] * y0 + gp.quicksum(r[i+1] * y[i] for i in range(n)), GRB.MAXIMIZE)\n",
    "\n",
    "    # Constraints\n",
    "    model.addConstr(y0 + gp.quicksum(y[i] for i in range(n)) == 1, \"probability_sum\")\n",
    "    for i in range(n):\n",
    "        model.addConstr(y[i] <= y0 * exp(mu[i+1]), f\"probability_{i}\")\n",
    "        model.addConstr(y[i] <= z[i], f\"binary_{i}\")\n",
    "    model.addConstr(sum(z[i] for i in range(n)) <= p, \"max_products\")\n",
    "\n",
    "    # Optimize\n",
    "    model.optimize()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# Solve the APC-MILP for the small instances\n",
    "small_r = instances_small_r\n",
    "small_mu = instances_small_mu\n",
    "\n",
    "n = len(small_r[0]) - 1 # Number of products\n",
    "n_small_instances = len(small_r) # Number of instances\n",
    "p_values = [1, n // 5, n // 2, n] # Number of products to be selected\n",
    "\n",
    "small_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1,n_small_instances+1)}\n",
    "small_instances_optimal_values = {instance: {p: None for p in p_values} for instance in  range(1,n_small_instances+1)}\n",
    "\n",
    "\n",
    "for i in range(n_small_instances):\n",
    "    print(f\"Small instance n°{i}\")\n",
    "    for p in p_values:\n",
    "        print(f\"p = {p}\")\n",
    "        model = solve_apc_milp(small_r[i], small_mu[i], p, n)\n",
    "        small_instances_runtimes[i+1][p] = model.Runtime\n",
    "        small_instances_optimal_values[i+1][p] = model.ObjVal\n",
    "        \n",
    "print(small_instances_runtimes)\n",
    "print(small_instances_optimal_values)\n",
    "# Solve the APC-MILP for the medium instances\n",
    "medium_r = instances_medium_r\n",
    "medium_mu = instances_medium_mu\n",
    "\n",
    "n = len(medium_r[0]) - 1 # Number of products\n",
    "n_medium_instances = len(medium_r) # Number of instances\n",
    "p_values = [1, n // 5, n // 2, n] # Number of products to be selected\n",
    "\n",
    "medium_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1,n_medium_instances+1)}\n",
    "medium_instances_optimal_values = {instance: {p: None for p in p_values} for instance in  range(1,n_medium_instances+1)}\n",
    "\n",
    "for i in range(n_medium_instances):\n",
    "    print(f\"Medium instance n°{i+1}\")\n",
    "    for p in p_values:\n",
    "        print(f\"p = {p}\")\n",
    "        model = solve_apc_milp(medium_r[i], medium_mu[i], p, n)\n",
    "        medium_instances_runtimes[i+1][p] = model.Runtime\n",
    "        medium_instances_optimal_values[i+1][p] = model.ObjVal\n",
    "        \n",
    "print(medium_instances_runtimes)\n",
    "print(medium_instances_optimal_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the APC-MILP for the large instances\n",
    "# large_r = instances_large_r\n",
    "# large_mu = instances_large_mu\n",
    "\n",
    "# n = len(large_r[0]) - 1 # Number of products\n",
    "# n_large_instances = len(large_r) # Number of instances\n",
    "# p_values = [1, n // 5, n // 2, n] # Number of products to be selected\n",
    "\n",
    "# large_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1, n_large_instances)}\n",
    "# large_instances_optimal_values = {instance: {p: None for p in p_values} for instance in  range(1, n_large_instances)}\n",
    "\n",
    "# for i in range(n_large_instances):\n",
    "#     print(f\"Large instance n°{i + 1}\")\n",
    "#     for p in p_values:\n",
    "#         print(f\"p = {p}\")\n",
    "#         model = solve_apc_milp(large_r[i], large_mu[i], p, n)\n",
    "#         large_instances_runtimes[i][p] = model.Runtime\n",
    "#         large_instances_optimal_values[i][p] = model.ObjVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 We now propose to use a Lagrangean Relaxation framework to (approximately) solve the practical model. Coming back to AP-IP, it is easy to cast yet another version of the practical model:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { (APC-IP) } \\max _{x \\in\\{0,1\\}^{n}} & \\frac{r_{0}+\\sum_{i=1}^{n} x_{i} r_{i} e^{\\mu_{i}}}{1+\\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}} \\\\\n",
    "\\text { s.t. } & \\sum_{i=1}^{n} x_{i} \\leqslant p,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### on which we will apply a Lagrangian algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.3.1 Pricing: after noticing that the constraint $\\sum_{i=1}^{n} x_{i} \\leqslant p$ is equivalent to\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} x_{i}}{1+\\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}} \\leqslant \\frac{p}{1+\\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}}\n",
    "$$\n",
    "\n",
    "##### prove that the pricing problem with a penalization $\\lambda \\geqslant 0$ can be recast as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "(\\operatorname{AP}-\\mathrm{L})(\\lambda) \\quad \\omega(\\lambda):=\\max _{y, y_{0} \\geqslant 0} & r_{0}(\\lambda) y_{0}+\\sum_{i=1}^{n} r_{i}(\\lambda) y_{i} & \\\\\n",
    "\\text { s.t. } & y_{0}+\\sum_{i=1}^{n} y_{i}=1 \\\\\n",
    "& y_{i} \\leqslant y_{0} e^{\\mu_{i}} & \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "##### Specify the values of $r_{0}(\\lambda)$ and $r_{i}(\\lambda)$ in function of $r_{0}, \\lambda, p$, the revenues $r_{j}$ and the utilities $\\mu_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To apply the Lagrangian relaxation, we introduce a penalization term $\\lambda \\geq 0$ for the constraint $\\sum_{i=1}^{n} x_{i} \\leq p$. The Lagrangian function for this problem is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = \\frac{r_{0} + \\sum_{i=1}^{n} x_{i} r_{i} e^{\\mu_{i}}}{1 + \\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}} - \\lambda \\left( \\sum_{i=1}^{n} x_{i} - p \\right)\n",
    "$$\n",
    "\n",
    "This penalization can be incorporated into the objective function, leading to a new revenue term for each product.\n",
    "\n",
    "### New Objective Function\n",
    "The new objective function with the penalization term becomes:\n",
    "\n",
    "$$\n",
    "\\omega(\\lambda) = \\max_{x \\in \\{0,1\\}^{n}} \\frac{r_{0} + \\sum_{i=1}^{n} x_{i} (r_{i} e^{\\mu_{i}} - \\lambda)}{1 + \\sum_{i=1}^{n} x_{i} e^{\\mu_{i}}}\n",
    "$$\n",
    "\n",
    "### New Revenues\n",
    "To match this objective with the form given in the problem, we define new revenues $r_{0}(\\lambda)$ and $r_{i}(\\lambda)$ as follows:\n",
    "\n",
    "- For the no-purchase option:\n",
    "\n",
    "$$\n",
    "r_{0}(\\lambda) = r_{0}\n",
    "$$\n",
    "\n",
    "- For each product $i$:\n",
    "\n",
    "$$\n",
    "r_{i}(\\lambda) = r_{i} e^{\\mu_{i}} - \\lambda\n",
    "$$\n",
    "\n",
    "### Reformulated Problem\n",
    "The reformulated problem can then be written as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "(\\operatorname{AP}-\\mathrm{L})(\\lambda) \\quad \\omega(\\lambda):=\\max _{y, y_{0} \\geqslant 0} & r_{0} y_{0} + \\sum_{i=1}^{n} (r_{i} e^{\\mu_{i}} - \\lambda) y_{i} & \\\\\n",
    "\\text { s.t. } & y_{0} + \\sum_{i=1}^{n} y_{i} = 1 \\\\\n",
    "& y_{i} \\leqslant y_{0} e^{\\mu_{i}} & \\forall i \\in \\mathcal{I}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, the values of $r_{0}(\\lambda)$ and $r_{i}(\\lambda)$ are specified as follows:\n",
    "\n",
    "- $r_{0}(\\lambda) = r_{0}$\n",
    "- $r_{i}(\\lambda) = r_{i} e^{\\mu_{i}} - \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.3.2 Recall that we want to find the best dual bound, i.e. solve the following univariate problem:\n",
    "\n",
    "$$\n",
    "\\min _{\\lambda \\geqslant 0} \\omega(\\lambda)\n",
    "$$\n",
    "\n",
    "##### and that the function $\\lambda \\rightarrow \\omega(\\lambda)$ is convex and piecewise linear. Design a binary search to find the optimal $\\lambda^{*}$. Hint: there is an optimal $\\lambda^{*}$ that is no greater than $\\max \\left\\{0, \\frac{r_{1}-r_{0}}{p}\\right\\}$ (why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To design a binary search to find the optimal $\\lambda^{*}$, we need to exploit the convexity and piecewise linearity of the function $\\lambda \\rightarrow \\omega(\\lambda)$. The hint suggests that the optimal $\\lambda^{*}$ is no greater than $\\max \\left\\{0, \\frac{r_{1}-r_{0}}{p}\\right\\}$. This is because $\\lambda$ effectively reduces the revenue contribution of each product, and the maximum penalization that can still maintain a feasible solution is influenced by the highest revenue product and the no-purchase option.\n",
    "\n",
    "### Binary Search Algorithm\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Set the lower bound of $\\lambda$: $\\lambda_{\\text{low}} = 0$\n",
    "   - Set the upper bound of $\\lambda$: $\\lambda_{\\text{high}} = \\max \\left\\{0, \\frac{r_{1} - r_{0}}{p}\\right\\}$\n",
    "\n",
    "2. **Binary Search**:\n",
    "   - While the difference between $\\lambda_{\\text{high}}$ and $\\lambda_{\\text{low}}$ is greater than a small tolerance (e.g., $\\epsilon = 10^{-6}$):\n",
    "     1. Compute the midpoint: $\\lambda_{\\text{mid}} = \\frac{\\lambda_{\\text{low}} + \\lambda_{\\text{high}}}{2}$\n",
    "     2. Evaluate $\\omega(\\lambda_{\\text{mid}})$ by solving the Lagrangian problem $(\\operatorname{AP}-\\mathrm{L})(\\lambda_{\\text{mid}})$\n",
    "     3. If the derivative of $\\omega(\\lambda)$ at $\\lambda_{\\text{mid}}$ is positive, it means we need a smaller $\\lambda$ to minimize $\\omega(\\lambda)$. Set $\\lambda_{\\text{high}} = \\lambda_{\\text{mid}}$\n",
    "     4. Otherwise, set $\\lambda_{\\text{low}} = \\lambda_{\\text{mid}}$\n",
    "\n",
    "3. **Termination**:\n",
    "   - The optimal $\\lambda^{*}$ is the midpoint when the loop terminates: $\\lambda^{*} = \\frac{\\lambda_{\\text{low}} + \\lambda_{\\text{high}}}{2}$\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTANCE 1 - p = 1\n",
      "INSTANCE 2 - p = 1\n",
      "INSTANCE 3 - p = 1\n",
      "INSTANCE 4 - p = 1\n",
      "INSTANCE 5 - p = 1\n",
      "INSTANCE 6 - p = 1\n",
      "INSTANCE 7 - p = 1\n",
      "INSTANCE 8 - p = 1\n",
      "INSTANCE 9 - p = 1\n",
      "INSTANCE 10 - p = 1\n",
      "INSTANCE 11 - p = 1\n",
      "INSTANCE 12 - p = 1\n",
      "INSTANCE 13 - p = 1\n",
      "INSTANCE 14 - p = 1\n",
      "INSTANCE 15 - p = 1\n",
      "INSTANCE 16 - p = 1\n",
      "INSTANCE 17 - p = 1\n",
      "INSTANCE 18 - p = 1\n",
      "INSTANCE 19 - p = 1\n",
      "INSTANCE 20 - p = 1\n",
      "INSTANCE 21 - p = 1\n",
      "INSTANCE 22 - p = 1\n",
      "INSTANCE 23 - p = 1\n",
      "INSTANCE 24 - p = 1\n",
      "INSTANCE 25 - p = 1\n",
      "INSTANCE 26 - p = 1\n",
      "INSTANCE 27 - p = 1\n",
      "INSTANCE 28 - p = 1\n",
      "INSTANCE 29 - p = 1\n",
      "INSTANCE 30 - p = 1\n",
      "INSTANCE 31 - p = 1\n",
      "INSTANCE 32 - p = 1\n",
      "INSTANCE 33 - p = 1\n",
      "INSTANCE 34 - p = 1\n",
      "INSTANCE 35 - p = 1\n",
      "INSTANCE 36 - p = 1\n",
      "INSTANCE 37 - p = 1\n",
      "INSTANCE 38 - p = 1\n",
      "INSTANCE 39 - p = 1\n",
      "INSTANCE 40 - p = 1\n",
      "INSTANCE 41 - p = 1\n",
      "INSTANCE 42 - p = 1\n",
      "INSTANCE 43 - p = 1\n",
      "INSTANCE 44 - p = 1\n",
      "INSTANCE 45 - p = 1\n",
      "INSTANCE 46 - p = 1\n",
      "INSTANCE 47 - p = 1\n",
      "INSTANCE 48 - p = 1\n",
      "INSTANCE 49 - p = 1\n",
      "INSTANCE 50 - p = 1\n",
      "INSTANCE 51 - p = 1\n",
      "INSTANCE 52 - p = 1\n",
      "INSTANCE 53 - p = 1\n",
      "INSTANCE 54 - p = 1\n",
      "INSTANCE 55 - p = 1\n",
      "INSTANCE 56 - p = 1\n",
      "INSTANCE 57 - p = 1\n",
      "INSTANCE 58 - p = 1\n",
      "INSTANCE 59 - p = 1\n",
      "INSTANCE 60 - p = 1\n",
      "INSTANCE 61 - p = 1\n",
      "INSTANCE 62 - p = 1\n",
      "INSTANCE 63 - p = 1\n",
      "INSTANCE 64 - p = 1\n",
      "INSTANCE 65 - p = 1\n",
      "INSTANCE 66 - p = 1\n",
      "INSTANCE 67 - p = 1\n",
      "INSTANCE 68 - p = 1\n",
      "INSTANCE 69 - p = 1\n",
      "INSTANCE 70 - p = 1\n",
      "INSTANCE 71 - p = 1\n",
      "INSTANCE 72 - p = 1\n",
      "INSTANCE 73 - p = 1\n",
      "INSTANCE 74 - p = 1\n",
      "INSTANCE 75 - p = 1\n",
      "INSTANCE 76 - p = 1\n",
      "INSTANCE 77 - p = 1\n",
      "INSTANCE 78 - p = 1\n",
      "INSTANCE 79 - p = 1\n",
      "INSTANCE 80 - p = 1\n",
      "INSTANCE 81 - p = 1\n",
      "INSTANCE 82 - p = 1\n",
      "INSTANCE 83 - p = 1\n",
      "INSTANCE 84 - p = 1\n",
      "INSTANCE 85 - p = 1\n",
      "INSTANCE 86 - p = 1\n",
      "INSTANCE 87 - p = 1\n",
      "INSTANCE 88 - p = 1\n",
      "INSTANCE 89 - p = 1\n",
      "INSTANCE 90 - p = 1\n",
      "INSTANCE 91 - p = 1\n",
      "INSTANCE 92 - p = 1\n",
      "INSTANCE 93 - p = 1\n",
      "INSTANCE 94 - p = 1\n",
      "INSTANCE 95 - p = 1\n",
      "INSTANCE 96 - p = 1\n",
      "INSTANCE 97 - p = 1\n",
      "INSTANCE 98 - p = 1\n",
      "INSTANCE 99 - p = 1\n",
      "INSTANCE 100 - p = 1\n",
      "INSTANCE 1 - p = 2\n",
      "INSTANCE 2 - p = 2\n",
      "INSTANCE 3 - p = 2\n",
      "INSTANCE 4 - p = 2\n",
      "INSTANCE 5 - p = 2\n",
      "INSTANCE 6 - p = 2\n",
      "INSTANCE 7 - p = 2\n",
      "INSTANCE 8 - p = 2\n",
      "INSTANCE 9 - p = 2\n",
      "INSTANCE 10 - p = 2\n",
      "INSTANCE 11 - p = 2\n",
      "INSTANCE 12 - p = 2\n",
      "INSTANCE 13 - p = 2\n",
      "INSTANCE 14 - p = 2\n",
      "INSTANCE 15 - p = 2\n",
      "INSTANCE 16 - p = 2\n",
      "INSTANCE 17 - p = 2\n",
      "INSTANCE 18 - p = 2\n",
      "INSTANCE 19 - p = 2\n",
      "INSTANCE 20 - p = 2\n",
      "INSTANCE 21 - p = 2\n",
      "INSTANCE 22 - p = 2\n",
      "INSTANCE 23 - p = 2\n",
      "INSTANCE 24 - p = 2\n",
      "INSTANCE 25 - p = 2\n",
      "INSTANCE 26 - p = 2\n",
      "INSTANCE 27 - p = 2\n",
      "INSTANCE 28 - p = 2\n",
      "INSTANCE 29 - p = 2\n",
      "INSTANCE 30 - p = 2\n",
      "INSTANCE 31 - p = 2\n",
      "INSTANCE 32 - p = 2\n",
      "INSTANCE 33 - p = 2\n",
      "INSTANCE 34 - p = 2\n",
      "INSTANCE 35 - p = 2\n",
      "INSTANCE 36 - p = 2\n",
      "INSTANCE 37 - p = 2\n",
      "INSTANCE 38 - p = 2\n",
      "INSTANCE 39 - p = 2\n",
      "INSTANCE 40 - p = 2\n",
      "INSTANCE 41 - p = 2\n",
      "INSTANCE 42 - p = 2\n",
      "INSTANCE 43 - p = 2\n",
      "INSTANCE 44 - p = 2\n",
      "INSTANCE 45 - p = 2\n",
      "INSTANCE 46 - p = 2\n",
      "INSTANCE 47 - p = 2\n",
      "INSTANCE 48 - p = 2\n",
      "INSTANCE 49 - p = 2\n",
      "INSTANCE 50 - p = 2\n",
      "INSTANCE 51 - p = 2\n",
      "INSTANCE 52 - p = 2\n",
      "INSTANCE 53 - p = 2\n",
      "INSTANCE 54 - p = 2\n",
      "INSTANCE 55 - p = 2\n",
      "INSTANCE 56 - p = 2\n",
      "INSTANCE 57 - p = 2\n",
      "INSTANCE 58 - p = 2\n",
      "INSTANCE 59 - p = 2\n",
      "INSTANCE 60 - p = 2\n",
      "INSTANCE 61 - p = 2\n",
      "INSTANCE 62 - p = 2\n",
      "INSTANCE 63 - p = 2\n",
      "INSTANCE 64 - p = 2\n",
      "INSTANCE 65 - p = 2\n",
      "INSTANCE 66 - p = 2\n",
      "INSTANCE 67 - p = 2\n",
      "INSTANCE 68 - p = 2\n",
      "INSTANCE 69 - p = 2\n",
      "INSTANCE 70 - p = 2\n",
      "INSTANCE 71 - p = 2\n",
      "INSTANCE 72 - p = 2\n",
      "INSTANCE 73 - p = 2\n",
      "INSTANCE 74 - p = 2\n",
      "INSTANCE 75 - p = 2\n",
      "INSTANCE 76 - p = 2\n",
      "INSTANCE 77 - p = 2\n",
      "INSTANCE 78 - p = 2\n",
      "INSTANCE 79 - p = 2\n",
      "INSTANCE 80 - p = 2\n",
      "INSTANCE 81 - p = 2\n",
      "INSTANCE 82 - p = 2\n",
      "INSTANCE 83 - p = 2\n",
      "INSTANCE 84 - p = 2\n",
      "INSTANCE 85 - p = 2\n",
      "INSTANCE 86 - p = 2\n",
      "INSTANCE 87 - p = 2\n",
      "INSTANCE 88 - p = 2\n",
      "INSTANCE 89 - p = 2\n",
      "INSTANCE 90 - p = 2\n",
      "INSTANCE 91 - p = 2\n",
      "INSTANCE 92 - p = 2\n",
      "INSTANCE 93 - p = 2\n",
      "INSTANCE 94 - p = 2\n",
      "INSTANCE 95 - p = 2\n",
      "INSTANCE 96 - p = 2\n",
      "INSTANCE 97 - p = 2\n",
      "INSTANCE 98 - p = 2\n",
      "INSTANCE 99 - p = 2\n",
      "INSTANCE 100 - p = 2\n",
      "INSTANCE 1 - p = 5\n",
      "INSTANCE 2 - p = 5\n",
      "INSTANCE 3 - p = 5\n",
      "INSTANCE 4 - p = 5\n",
      "INSTANCE 5 - p = 5\n",
      "INSTANCE 6 - p = 5\n",
      "INSTANCE 7 - p = 5\n",
      "INSTANCE 8 - p = 5\n",
      "INSTANCE 9 - p = 5\n",
      "INSTANCE 10 - p = 5\n",
      "INSTANCE 11 - p = 5\n",
      "INSTANCE 12 - p = 5\n",
      "INSTANCE 13 - p = 5\n",
      "INSTANCE 14 - p = 5\n",
      "INSTANCE 15 - p = 5\n",
      "INSTANCE 16 - p = 5\n",
      "INSTANCE 17 - p = 5\n",
      "INSTANCE 18 - p = 5\n",
      "INSTANCE 19 - p = 5\n",
      "INSTANCE 20 - p = 5\n",
      "INSTANCE 21 - p = 5\n",
      "INSTANCE 22 - p = 5\n",
      "INSTANCE 23 - p = 5\n",
      "INSTANCE 24 - p = 5\n",
      "INSTANCE 25 - p = 5\n",
      "INSTANCE 26 - p = 5\n",
      "INSTANCE 27 - p = 5\n",
      "INSTANCE 28 - p = 5\n",
      "INSTANCE 29 - p = 5\n",
      "INSTANCE 30 - p = 5\n",
      "INSTANCE 31 - p = 5\n",
      "INSTANCE 32 - p = 5\n",
      "INSTANCE 33 - p = 5\n",
      "INSTANCE 34 - p = 5\n",
      "INSTANCE 35 - p = 5\n",
      "INSTANCE 36 - p = 5\n",
      "INSTANCE 37 - p = 5\n",
      "INSTANCE 38 - p = 5\n",
      "INSTANCE 39 - p = 5\n",
      "INSTANCE 40 - p = 5\n",
      "INSTANCE 41 - p = 5\n",
      "INSTANCE 42 - p = 5\n",
      "INSTANCE 43 - p = 5\n",
      "INSTANCE 44 - p = 5\n",
      "INSTANCE 45 - p = 5\n",
      "INSTANCE 46 - p = 5\n",
      "INSTANCE 47 - p = 5\n",
      "INSTANCE 48 - p = 5\n",
      "INSTANCE 49 - p = 5\n",
      "INSTANCE 50 - p = 5\n",
      "INSTANCE 51 - p = 5\n",
      "INSTANCE 52 - p = 5\n",
      "INSTANCE 53 - p = 5\n",
      "INSTANCE 54 - p = 5\n",
      "INSTANCE 55 - p = 5\n",
      "INSTANCE 56 - p = 5\n",
      "INSTANCE 57 - p = 5\n",
      "INSTANCE 58 - p = 5\n",
      "INSTANCE 59 - p = 5\n",
      "INSTANCE 60 - p = 5\n",
      "INSTANCE 61 - p = 5\n",
      "INSTANCE 62 - p = 5\n",
      "INSTANCE 63 - p = 5\n",
      "INSTANCE 64 - p = 5\n",
      "INSTANCE 65 - p = 5\n",
      "INSTANCE 66 - p = 5\n",
      "INSTANCE 67 - p = 5\n",
      "INSTANCE 68 - p = 5\n",
      "INSTANCE 69 - p = 5\n",
      "INSTANCE 70 - p = 5\n",
      "INSTANCE 71 - p = 5\n",
      "INSTANCE 72 - p = 5\n",
      "INSTANCE 73 - p = 5\n",
      "INSTANCE 74 - p = 5\n",
      "INSTANCE 75 - p = 5\n",
      "INSTANCE 76 - p = 5\n",
      "INSTANCE 77 - p = 5\n",
      "INSTANCE 78 - p = 5\n",
      "INSTANCE 79 - p = 5\n",
      "INSTANCE 80 - p = 5\n",
      "INSTANCE 81 - p = 5\n",
      "INSTANCE 82 - p = 5\n",
      "INSTANCE 83 - p = 5\n",
      "INSTANCE 84 - p = 5\n",
      "INSTANCE 85 - p = 5\n",
      "INSTANCE 86 - p = 5\n",
      "INSTANCE 87 - p = 5\n",
      "INSTANCE 88 - p = 5\n",
      "INSTANCE 89 - p = 5\n",
      "INSTANCE 90 - p = 5\n",
      "INSTANCE 91 - p = 5\n",
      "INSTANCE 92 - p = 5\n",
      "INSTANCE 93 - p = 5\n",
      "INSTANCE 94 - p = 5\n",
      "INSTANCE 95 - p = 5\n",
      "INSTANCE 96 - p = 5\n",
      "INSTANCE 97 - p = 5\n",
      "INSTANCE 98 - p = 5\n",
      "INSTANCE 99 - p = 5\n",
      "INSTANCE 100 - p = 5\n",
      "INSTANCE 1 - p = 10\n",
      "INSTANCE 2 - p = 10\n",
      "INSTANCE 3 - p = 10\n",
      "INSTANCE 4 - p = 10\n",
      "INSTANCE 5 - p = 10\n",
      "INSTANCE 6 - p = 10\n",
      "INSTANCE 7 - p = 10\n",
      "INSTANCE 8 - p = 10\n",
      "INSTANCE 9 - p = 10\n",
      "INSTANCE 10 - p = 10\n",
      "INSTANCE 11 - p = 10\n",
      "INSTANCE 12 - p = 10\n",
      "INSTANCE 13 - p = 10\n",
      "INSTANCE 14 - p = 10\n",
      "INSTANCE 15 - p = 10\n",
      "INSTANCE 16 - p = 10\n",
      "INSTANCE 17 - p = 10\n",
      "INSTANCE 18 - p = 10\n",
      "INSTANCE 19 - p = 10\n",
      "INSTANCE 20 - p = 10\n",
      "INSTANCE 21 - p = 10\n",
      "INSTANCE 22 - p = 10\n",
      "INSTANCE 23 - p = 10\n",
      "INSTANCE 24 - p = 10\n",
      "INSTANCE 25 - p = 10\n",
      "INSTANCE 26 - p = 10\n",
      "INSTANCE 27 - p = 10\n",
      "INSTANCE 28 - p = 10\n",
      "INSTANCE 29 - p = 10\n",
      "INSTANCE 30 - p = 10\n",
      "INSTANCE 31 - p = 10\n",
      "INSTANCE 32 - p = 10\n",
      "INSTANCE 33 - p = 10\n",
      "INSTANCE 34 - p = 10\n",
      "INSTANCE 35 - p = 10\n",
      "INSTANCE 36 - p = 10\n",
      "INSTANCE 37 - p = 10\n",
      "INSTANCE 38 - p = 10\n",
      "INSTANCE 39 - p = 10\n",
      "INSTANCE 40 - p = 10\n",
      "INSTANCE 41 - p = 10\n",
      "INSTANCE 42 - p = 10\n",
      "INSTANCE 43 - p = 10\n",
      "INSTANCE 44 - p = 10\n",
      "INSTANCE 45 - p = 10\n",
      "INSTANCE 46 - p = 10\n",
      "INSTANCE 47 - p = 10\n",
      "INSTANCE 48 - p = 10\n",
      "INSTANCE 49 - p = 10\n",
      "INSTANCE 50 - p = 10\n",
      "INSTANCE 51 - p = 10\n",
      "INSTANCE 52 - p = 10\n",
      "INSTANCE 53 - p = 10\n",
      "INSTANCE 54 - p = 10\n",
      "INSTANCE 55 - p = 10\n",
      "INSTANCE 56 - p = 10\n",
      "INSTANCE 57 - p = 10\n",
      "INSTANCE 58 - p = 10\n",
      "INSTANCE 59 - p = 10\n",
      "INSTANCE 60 - p = 10\n",
      "INSTANCE 61 - p = 10\n",
      "INSTANCE 62 - p = 10\n",
      "INSTANCE 63 - p = 10\n",
      "INSTANCE 64 - p = 10\n",
      "INSTANCE 65 - p = 10\n",
      "INSTANCE 66 - p = 10\n",
      "INSTANCE 67 - p = 10\n",
      "INSTANCE 68 - p = 10\n",
      "INSTANCE 69 - p = 10\n",
      "INSTANCE 70 - p = 10\n",
      "INSTANCE 71 - p = 10\n",
      "INSTANCE 72 - p = 10\n",
      "INSTANCE 73 - p = 10\n",
      "INSTANCE 74 - p = 10\n",
      "INSTANCE 75 - p = 10\n",
      "INSTANCE 76 - p = 10\n",
      "INSTANCE 77 - p = 10\n",
      "INSTANCE 78 - p = 10\n",
      "INSTANCE 79 - p = 10\n",
      "INSTANCE 80 - p = 10\n",
      "INSTANCE 81 - p = 10\n",
      "INSTANCE 82 - p = 10\n",
      "INSTANCE 83 - p = 10\n",
      "INSTANCE 84 - p = 10\n",
      "INSTANCE 85 - p = 10\n",
      "INSTANCE 86 - p = 10\n",
      "INSTANCE 87 - p = 10\n",
      "INSTANCE 88 - p = 10\n",
      "INSTANCE 89 - p = 10\n",
      "INSTANCE 90 - p = 10\n",
      "INSTANCE 91 - p = 10\n",
      "INSTANCE 92 - p = 10\n",
      "INSTANCE 93 - p = 10\n",
      "INSTANCE 94 - p = 10\n",
      "INSTANCE 95 - p = 10\n",
      "INSTANCE 96 - p = 10\n",
      "INSTANCE 97 - p = 10\n",
      "INSTANCE 98 - p = 10\n",
      "INSTANCE 99 - p = 10\n",
      "INSTANCE 100 - p = 10\n",
      "{1: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0010001659393310547}, 2: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 3: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 4: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 5: {1: 0.0, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 6: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 7: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 8: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 9: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 10: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0009999275207519531}, 11: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 12: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 13: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 14: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 15: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 16: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 17: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 18: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.002000093460083008}, 19: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0010001659393310547}, 20: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 21: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0010001659393310547, 10: 0.0010001659393310547}, 22: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 23: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 24: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 25: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 26: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 27: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 28: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0009999275207519531}, 29: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 30: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 31: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 32: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 33: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0010001659393310547}, 34: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 35: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 36: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 37: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 38: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 39: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 40: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.002000093460083008}, 41: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 42: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 43: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0009999275207519531, 10: 0.0}, 44: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 45: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 46: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 47: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 48: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 49: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 50: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 51: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 52: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0019998550415039062}, 53: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 54: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 55: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 56: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 57: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 58: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 59: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 60: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0019998550415039062}, 61: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 62: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 63: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 64: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 65: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0009999275207519531}, 66: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0009999275207519531, 10: 0.0}, 67: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 68: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 69: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 70: {1: 0.0, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0010001659393310547}, 71: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 72: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 73: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 74: {1: 0.0009999275207519531, 2: 0.002000093460083008, 5: 0.0, 10: 0.0}, 75: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0010001659393310547, 10: 0.0}, 76: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 77: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 78: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 79: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 80: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 81: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 82: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 83: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 84: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 85: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 86: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0010001659393310547}, 87: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 88: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0009999275207519531, 10: 0.0}, 89: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 90: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 91: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 92: {1: 0.0009999275207519531, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 93: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 94: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 95: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.002000093460083008}, 96: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 97: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 98: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 99: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 100: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}}\n",
      "{1: {1: 0.8058279149620388, 2: 0.4029137653565813, 5: 0.161165352443082, 10: 0.08058252252199052}, 2: {1: 0.9107064276464418, 2: 0.45535299669378493, 5: 0.18214102497396517, 10: 0.09107033878343379}, 3: {1: 0.8127163839059393, 2: 0.4063579981861918, 5: 0.16254304426105443, 10: 0.08127136711710493}, 4: {1: 0.7750507880097579, 2: 0.38752520921828326, 5: 0.15500993585803674, 10: 0.07750482009974179}, 5: {1: 0.9497640165574673, 2: 0.4748817818372384, 5: 0.1899525315816991, 10: 0.09497608463765334}, 6: {1: 0.9237749897784464, 2: 0.4618872746439977, 5: 0.1847547336614187, 10: 0.09237719063452896}, 7: {1: 0.8587323492911348, 2: 0.42936596990772324, 5: 0.171746224172814, 10: 0.08587294829613165}, 8: {1: 0.9145729101789692, 2: 0.4572862370382069, 5: 0.18291432037426064, 10: 0.09145698574610815}, 9: {1: 0.8715342127210708, 2: 0.43576689857048767, 5: 0.1743065931961569, 10: 0.08715313036604028}, 10: {1: 0.8692007138652431, 2: 0.4346001496989236, 5: 0.17383989409261108, 10: 0.08691978125934716}, 11: {1: 0.9962870948043572, 2: 0.4981433098687118, 5: 0.19925713392071132, 10: 0.09962837693358223}, 12: {1: 0.8980013009387904, 2: 0.4490004363690989, 5: 0.17960000326740252, 10: 0.08979983035346423}, 13: {1: 0.43709941098527316, 2: 0.2185494970671968, 5: 0.08741963208652692, 10: 0.04370964930291166}, 14: {1: 0.8171481566443859, 2: 0.40857388349879764, 5: 0.16342939754080282, 10: 0.08171454291168515}, 15: {1: 0.8990746219489285, 2: 0.44953709661826813, 5: 0.1798146671623504, 10: 0.08990716209621834}, 16: {1: 0.8158676239077174, 2: 0.4079336174357663, 5: 0.1631732913598326, 10: 0.08158649006544239}, 17: {1: 0.7621764643209733, 2: 0.38108805044337035, 5: 0.1524350748036551, 10: 0.07621739202813452}, 18: {1: 0.8048910475000661, 2: 0.4024453318489617, 5: 0.16097797921872758, 10: 0.08048883608850668}, 19: {1: 0.8958898868889673, 2: 0.44794472984758793, 5: 0.1791777210615186, 10: 0.08958868965324274}, 20: {1: 0.9564256141195506, 2: 0.47821257903003067, 5: 0.19128484918821653, 10: 0.09564224217031253}, 21: {1: 0.8732556429612781, 2: 0.4366276132801702, 5: 0.174650878751693, 10: 0.0873252728154714}, 22: {1: 0.7271274746348392, 2: 0.3635635639566376, 5: 0.14542528689402945, 10: 0.07271250475838914}, 23: {1: 0.9343039415991227, 2: 0.4671517480440369, 5: 0.18686052101319522, 10: 0.09343008230217809}, 24: {1: 0.734320251801962, 2: 0.3671599508253065, 5: 0.146863840269583, 10: 0.07343178007425187}, 25: {1: 0.6874589354259062, 2: 0.3437293038098924, 5: 0.13749159040150838, 10: 0.0687456640783056}, 26: {1: 0.8301828435558354, 2: 0.4150912238468094, 5: 0.1660363311938371, 10: 0.08301800725203187}, 27: {1: 0.6195623461400961, 2: 0.30978102535480334, 5: 0.12391205562533414, 10: 0.061955791468275595}, 28: {1: 0.6654265753111064, 2: 0.33271312900541894, 5: 0.13308512468206019, 10: 0.0665424354209227}, 29: {1: 0.9535059092422582, 2: 0.4767527272874967, 5: 0.19070090904809273, 10: 0.0953502726571404}, 30: {1: 0.6310129327213245, 2: 0.3155063159153837, 5: 0.12620216529748493, 10: 0.06310084193629678}, 31: {1: 0.9061752548186259, 2: 0.45308741136019326, 5: 0.18123479170478154, 10: 0.09061722301309502}, 32: {1: 0.8391820980082034, 2: 0.41959084892740295, 5: 0.1678361795096021, 10: 0.083917929693442}, 33: {1: 0.8933521033774079, 2: 0.4466758386968633, 5: 0.17867016508527278, 10: 0.08933491214916384}, 34: {1: 0.671913497204969, 2: 0.3359565884057468, 5: 0.13438250720490857, 10: 0.06719112544506416}, 35: {1: 0.8620712664744383, 2: 0.4310354277033147, 5: 0.17241400665420237, 10: 0.08620683889997763}, 36: {1: 0.858356477939488, 2: 0.4291780343215146, 5: 0.17167105001002228, 10: 0.0858353612864276}, 37: {1: 0.9614948482403842, 2: 0.4807471948818473, 5: 0.19229869456206306, 10: 0.09614916389035565}, 38: {1: 0.9304151182761373, 2: 0.4652073373097124, 5: 0.18608275746119995, 10: 0.09304120126791499}, 39: {1: 0.9194921593998749, 2: 0.4597458604758188, 5: 0.1838981688110326, 10: 0.09194890902622135}, 40: {1: 0.8641363810408709, 2: 0.43206798449416906, 5: 0.17282702897665453, 10: 0.08641334966731418}, 41: {1: 0.9637307430501657, 2: 0.4818651417536589, 5: 0.1927458728843244, 10: 0.09637275262502304}, 42: {1: 0.3580445633737938, 2: 0.17902211095778198, 5: 0.07160870779982084, 10: 0.03580421731661848}, 43: {1: 0.9108631798835303, 2: 0.4554313727749565, 5: 0.18217237537653563, 10: 0.0910860139548209}, 44: {1: 0.6944445697781254, 2: 0.3472221193204962, 5: 0.13888871527334523, 10: 0.06944422518181939}, 45: {1: 0.8519550829366842, 2: 0.4259773383463249, 5: 0.1703907728409162, 10: 0.08519522392284434}, 46: {1: 0.8876290411411101, 2: 0.4438143089431994, 5: 0.17752555427539524, 10: 0.08876260783581308}, 47: {1: 0.7847146478482256, 2: 0.3923571368334723, 5: 0.1569427050608765, 10: 0.0784712028579258}, 48: {1: 0.7891450202417487, 2: 0.3945723219739502, 5: 0.15782877827204078, 10: 0.07891423861848107}, 49: {1: 0.8193787437089526, 2: 0.40968917649926734, 5: 0.16387551431553976, 10: 0.08193760087360269}, 50: {1: 0.8673653163569224, 2: 0.4336824513823563, 5: 0.1734728151160586, 10: 0.08673624212114539}, 51: {1: 0.7468479829768995, 2: 0.37342381342592995, 5: 0.14936938292035612, 10: 0.07468454901016222}, 52: {1: 0.7774542611301087, 2: 0.3887269452054257, 5: 0.15549062979446737, 10: 0.07774516660953078}, 53: {1: 0.8534539913403028, 2: 0.42672679219076637, 5: 0.1706905540927986, 10: 0.08534511426289132}, 54: {1: 0.9178062459433174, 2: 0.4589029041494933, 5: 0.18356098660206505, 10: 0.09178031824330021}, 55: {1: 0.9368680341989042, 2: 0.4684337937326001, 5: 0.18737333879955842, 10: 0.09368649070629759}, 56: {1: 0.8792732618573127, 2: 0.4396364212934747, 5: 0.1758544008092445, 10: 0.08792703269647689}, 57: {1: 0.7755588900919417, 2: 0.3877792601382341, 5: 0.15511155612910427, 10: 0.07755563013836272}, 58: {1: 0.9151277221576131, 2: 0.45756364289525125, 5: 0.18302528261125628, 10: 0.09151246675878394}, 59: {1: 0.7843309975375967, 2: 0.3921653117696272, 5: 0.15686597510851397, 10: 0.07843283795492006}, 60: {1: 0.9823711134390187, 2: 0.49118532250387265, 5: 0.1964739416290397, 10: 0.09823678344201048}, 61: {1: 0.7369288078033407, 2: 0.3684642282040673, 5: 0.14738555072354448, 10: 0.07369263480368982}, 62: {1: 0.9257587008711126, 2: 0.462879129717377, 5: 0.1851514753124074, 10: 0.09257556108166025}, 63: {1: 0.9307001919921845, 2: 0.465349874099769, 5: 0.1861397721228491, 10: 0.09306970854436603}, 64: {1: 0.951768030146676, 2: 0.4758837881540484, 5: 0.19035333372618768, 10: 0.09517648532766218}, 65: {1: 0.7156091456892356, 2: 0.35780440223002075, 5: 0.14312162440033066, 10: 0.07156067570848768}, 66: {1: 0.9083722169741286, 2: 0.4541858919141478, 5: 0.1816741835073259, 10: 0.09083691849532971}, 67: {1: 0.7967194237337744, 2: 0.39835952191408364, 5: 0.15934365680339063, 10: 0.0796716764394525}, 68: {1: 0.6820346619013944, 2: 0.3410171683408847, 5: 0.13640673724850383, 10: 0.0682032385364019}, 69: {1: 0.8818924216131778, 2: 0.44094600054695055, 5: 0.17637823201106956, 10: 0.0881889477978241}, 70: {1: 0.9448717319854062, 2: 0.4724356407176199, 5: 0.1889740760669814, 10: 0.09448685781342414}, 71: {1: 0.9262878236936898, 2: 0.4631436910025128, 5: 0.18525729972553945, 10: 0.09262847318730408}, 72: {1: 0.9191359473836339, 2: 0.4595677545526259, 5: 0.1838269265096975, 10: 0.09191328794349589}, 73: {1: 0.4656546602613404, 2: 0.23282710808901355, 5: 0.09313066560228009, 10: 0.04656515516781472}, 74: {1: 0.9188588170696986, 2: 0.4594291894617313, 5: 0.18377150052619812, 10: 0.09188557500460463}, 75: {1: 0.7889386637208556, 2: 0.3944691437627029, 5: 0.15778750702690125, 10: 0.07889360303527071}, 76: {1: 0.9189745634615503, 2: 0.459487062630061, 5: 0.1837946497714531, 10: 0.09189714960515524}, 77: {1: 0.8583122317749144, 2: 0.4291559112497769, 5: 0.17166220078976652, 10: 0.08583093668473901}, 78: {1: 0.8824583299059408, 2: 0.44122895455840894, 5: 0.1764914135077144, 10: 0.08824553843820804}, 79: {1: 0.8008927572739046, 2: 0.4004461876891481, 5: 0.16017832231741583, 10: 0.08008900840046451}, 80: {1: 0.5986443949003617, 2: 0.2993220547221668, 5: 0.11972847934163294, 10: 0.05986401130599395}, 81: {1: 0.5494449415171936, 2: 0.2747223397606521, 5: 0.10988862150919354, 10: 0.05494410115788522}, 82: {1: 0.9601805302081667, 2: 0.4800900361790965, 5: 0.19203583133164914, 10: 0.09601773252583509}, 83: {1: 0.808205935788481, 2: 0.40410277520283777, 5: 0.16164095592801295, 10: 0.0808203238108843}, 84: {1: 0.7520026416690153, 2: 0.37600114154302083, 5: 0.15040031318401886, 10: 0.07520001315882}, 85: {1: 0.8741148836397199, 2: 0.437057233414532, 5: 0.1748227266415505, 10: 0.08741119659651292}, 86: {1: 0.7922694662754965, 2: 0.3961345442458978, 5: 0.1584536665848787, 10: 0.07922668217895897}, 87: {1: 0.8065272942554835, 2: 0.40326345483655857, 5: 0.16130522810167686, 10: 0.08065246021789188}, 88: {1: 0.6436101775735945, 2: 0.32180493533810006, 5: 0.12872160585836678, 10: 0.06436055741126787}, 89: {1: 0.9936785669768687, 2: 0.4968390465768894, 5: 0.19873542910151976, 10: 0.09936752502152389}, 90: {1: 0.8505443761997715, 2: 0.4252719853142074, 5: 0.1701086318971403, 10: 0.08505415372002748}, 91: {1: 0.9236799971927805, 2: 0.46183977837381274, 5: 0.1847357351714631, 10: 0.09236769140766957}, 92: {1: 0.9128050222760014, 2: 0.4564022935082205, 5: 0.18256074329946403, 10: 0.09128019754590788}, 93: {1: 0.7887572824304372, 2: 0.3943784531607384, 5: 0.1577512308207112, 10: 0.07887546496677145}, 94: {1: 0.7902586648760297, 2: 0.395129144025577, 5: 0.15805150688028052, 10: 0.07902560271018999}, 95: {1: 0.9397240869499708, 2: 0.46986181942719696, 5: 0.18794454853264808, 10: 0.09397209502809334}, 96: {1: 0.7305550214618758, 2: 0.3652773365529647, 5: 0.14611079527880727, 10: 0.07305525829702506}, 97: {1: 0.9319176948916732, 2: 0.46595862525923804, 5: 0.18638327235441632, 10: 0.09319145842792928}, 98: {1: 0.8353209099742959, 2: 0.4176602558310285, 5: 0.16706394300751587, 10: 0.08353181217886241}, 99: {1: 0.7162094021861802, 2: 0.3581045303353807, 5: 0.14324167552798472, 10: 0.07162070115782482}, 100: {1: 0.7790834744727699, 2: 0.38954155148832137, 5: 0.1558164719968777, 10: 0.077908087399988}}\n",
      "INSTANCE 1 - p = 1\n",
      "INSTANCE 2 - p = 1\n",
      "INSTANCE 3 - p = 1\n",
      "INSTANCE 4 - p = 1\n",
      "INSTANCE 5 - p = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(small_r)):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTANCE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - p = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m     lambda_star,runtime \u001b[38;5;241m=\u001b[39m \u001b[43mfind_optimal_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmedium_r\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmedium_mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     medium_instances_runtimes[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][p] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m     78\u001b[0m     medium_instances_lambda[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][p] \u001b[38;5;241m=\u001b[39m lambda_star\n",
      "Cell \u001b[1;32mIn[23], line 32\u001b[0m, in \u001b[0;36mfind_optimal_lambda\u001b[1;34m(r, mu, p)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m lambda_high \u001b[38;5;241m-\u001b[39m lambda_low \u001b[38;5;241m>\u001b[39m epsilon:\n\u001b[0;32m     31\u001b[0m     lambda_mid \u001b[38;5;241m=\u001b[39m (lambda_low \u001b[38;5;241m+\u001b[39m lambda_high) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 32\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43msolve_lagrangian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     obj_val_mid \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mObjVal\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# We evaluate the function value at lambda_mid + epsilon to determine the direction of the search\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36msolve_lagrangian\u001b[1;34m(r, mu, lambda_val)\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msetObjective(r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m y0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m((r[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m exp(mu[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m lambda_val) \u001b[38;5;241m*\u001b[39m y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y))), GRB\u001b[38;5;241m.\u001b[39mMAXIMIZE)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Constraints\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddConstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability_sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39maddConstr(y[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m*\u001b[39m exp(mu[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\model.pxi:3772\u001b[0m, in \u001b[0;36mgurobipy.Model.addConstr\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\linexpr.pxi:506\u001b[0m, in \u001b[0;36mgurobipy.LinExpr.__sub__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\linexpr.pxi:480\u001b[0m, in \u001b[0;36mgurobipy.LinExpr.__add__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\linexpr.pxi:48\u001b[0m, in \u001b[0;36mgurobipy.LinExpr.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gurobipy import Model, GRB\n",
    "from math import exp\n",
    "\n",
    "def solve_lagrangian(r, mu, lambda_val):\n",
    "    model = Model(\"Lagrangian\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "    # Variables\n",
    "    y0 = model.addVar(vtype=GRB.CONTINUOUS, name=\"y0\")\n",
    "    y = [model.addVar(vtype=GRB.CONTINUOUS, name=f\"y_{i}\") for i in range(len(mu) - 1)]\n",
    "\n",
    "    # Objective function\n",
    "    model.setObjective(r[0] * y0 + sum((r[i + 1] * exp(mu[i + 1]) - lambda_val) * y[i] for i in range(len(y))), GRB.MAXIMIZE)\n",
    "\n",
    "    # Constraints\n",
    "    model.addConstr(y0 + sum(y[i] for i in range(len(y))) == 1, \"probability_sum\")\n",
    "    for i in range(len(y)):\n",
    "        model.addConstr(y[i] <= y0 * exp(mu[i + 1]), f\"probability_{i}\")\n",
    "\n",
    "    # Optimize\n",
    "    model.optimize()\n",
    "\n",
    "    return model\n",
    "\n",
    "def find_optimal_lambda(r, mu, p):\n",
    "    lambda_low = 0\n",
    "    lambda_high = max(0, (r[1] - r[0]) / p)\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    while lambda_high - lambda_low > epsilon:\n",
    "        lambda_mid = (lambda_low + lambda_high) / 2\n",
    "        mod = solve_lagrangian(r, mu, lambda_mid)\n",
    "        obj_val_mid = mod.ObjVal\n",
    "\n",
    "        # We evaluate the function value at lambda_mid + epsilon to determine the direction of the search\n",
    "        if obj_val_mid < (solve_lagrangian(r, mu, lambda_mid + epsilon).ObjVal):\n",
    "            lambda_high = lambda_mid\n",
    "        else:\n",
    "            lambda_low = lambda_mid\n",
    "\n",
    "    return (lambda_low + lambda_high) / 2, mod.Runtime\n",
    "\n",
    "# Small instances\n",
    "small_r = instances_small_r\n",
    "small_mu = instances_small_mu\n",
    "n = len(small_r[0]) -1  # number of products\n",
    "n_small_instances = len(small_r) # Number of instances\n",
    "p_values = [1, n // 5, n // 2, n]\n",
    "\n",
    "small_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1,n_small_instances+1)}\n",
    "small_instances_lambda = {instance: {p: None for p in p_values} for instance in  range(1,n_small_instances+1)}\n",
    "\n",
    "for p in p_values:\n",
    "    for i in range(len(small_r)):\n",
    "        print(f\"INSTANCE {i+1} - p = {p}\")\n",
    "        lambda_star,runtime = find_optimal_lambda(small_r[i],small_mu[i], p)\n",
    "        small_instances_runtimes[i+1][p] = runtime\n",
    "        small_instances_lambda[i+1][p] = lambda_star\n",
    "        \n",
    "print(small_instances_runtimes)\n",
    "print(small_instances_lambda)\n",
    "\n",
    "# medium instances\n",
    "medium_r = instances_medium_r\n",
    "medium_mu = instances_medium_mu\n",
    "n = len(medium_r[0]) -1  # number of products\n",
    "n_medium_instances = len(medium_r) # Number of instances\n",
    "p_values = [1, n // 5, n // 2, n]\n",
    "\n",
    "medium_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1,n_medium_instances+1)}\n",
    "medium_instances_lambda = {instance: {p: None for p in p_values} for instance in  range(1,n_medium_instances+1)}\n",
    "\n",
    "for p in p_values:\n",
    "    for i in range(len(small_r)):\n",
    "        print(f\"INSTANCE {i+1} - p = {p}\")\n",
    "        lambda_star,runtime = find_optimal_lambda(medium_r[i], medium_mu[i], p)\n",
    "        medium_instances_runtimes[i+1][p] = runtime\n",
    "        medium_instances_lambda[i+1][p] = lambda_star\n",
    "\n",
    "print(medium_instances_runtimes)\n",
    "print(medium_instances_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.3.2 Heuristic: whenever some $\\lambda$ provides a feasible solution for our practical problem, we compare it with the best solution so far and keep the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of using Lagrangian relaxation, we can employ a heuristic to improve our practical solution iteratively. Whenever a specific value of $\\lambda$ yields a feasible solution for the practical problem, we compare this solution with the best one obtained so far and retain the better one. Here’s how this can be implemented:\n",
    "\n",
    "### Heuristic Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Set the best objective value: `best_obj_val = -inf`\n",
    "   - Set the best solution: `best_solution = None`\n",
    "\n",
    "2. **Binary Search with Heuristic**:\n",
    "   - While performing the binary search to find the optimal $\\lambda$, keep track of feasible solutions.\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - After solving the Lagrangian problem $(\\operatorname{AP}-\\mathrm{L})(\\lambda_{\\text{mid}})$, check if the solution is feasible for the original practical problem.\n",
    "   - If feasible, compare the objective value with the best one obtained so far.\n",
    "   - Update `best_obj_val` and `best_solution` if the current solution is better.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTANCE 1 - p = 1\n",
      "INSTANCE 2 - p = 1\n",
      "INSTANCE 3 - p = 1\n",
      "INSTANCE 4 - p = 1\n",
      "INSTANCE 5 - p = 1\n",
      "INSTANCE 6 - p = 1\n",
      "INSTANCE 7 - p = 1\n",
      "INSTANCE 8 - p = 1\n",
      "INSTANCE 9 - p = 1\n",
      "INSTANCE 10 - p = 1\n",
      "INSTANCE 11 - p = 1\n",
      "INSTANCE 12 - p = 1\n",
      "INSTANCE 13 - p = 1\n",
      "INSTANCE 14 - p = 1\n",
      "INSTANCE 15 - p = 1\n",
      "INSTANCE 16 - p = 1\n",
      "INSTANCE 17 - p = 1\n",
      "INSTANCE 18 - p = 1\n",
      "INSTANCE 19 - p = 1\n",
      "INSTANCE 20 - p = 1\n",
      "INSTANCE 21 - p = 1\n",
      "INSTANCE 22 - p = 1\n",
      "INSTANCE 23 - p = 1\n",
      "INSTANCE 24 - p = 1\n",
      "INSTANCE 25 - p = 1\n",
      "INSTANCE 26 - p = 1\n",
      "INSTANCE 27 - p = 1\n",
      "INSTANCE 28 - p = 1\n",
      "INSTANCE 29 - p = 1\n",
      "INSTANCE 30 - p = 1\n",
      "INSTANCE 31 - p = 1\n",
      "INSTANCE 32 - p = 1\n",
      "INSTANCE 33 - p = 1\n",
      "INSTANCE 34 - p = 1\n",
      "INSTANCE 35 - p = 1\n",
      "INSTANCE 36 - p = 1\n",
      "INSTANCE 37 - p = 1\n",
      "INSTANCE 38 - p = 1\n",
      "INSTANCE 39 - p = 1\n",
      "INSTANCE 40 - p = 1\n",
      "INSTANCE 41 - p = 1\n",
      "INSTANCE 42 - p = 1\n",
      "INSTANCE 43 - p = 1\n",
      "INSTANCE 44 - p = 1\n",
      "INSTANCE 45 - p = 1\n",
      "INSTANCE 46 - p = 1\n",
      "INSTANCE 47 - p = 1\n",
      "INSTANCE 48 - p = 1\n",
      "INSTANCE 49 - p = 1\n",
      "INSTANCE 50 - p = 1\n",
      "INSTANCE 51 - p = 1\n",
      "INSTANCE 52 - p = 1\n",
      "INSTANCE 53 - p = 1\n",
      "INSTANCE 54 - p = 1\n",
      "INSTANCE 55 - p = 1\n",
      "INSTANCE 56 - p = 1\n",
      "INSTANCE 57 - p = 1\n",
      "INSTANCE 58 - p = 1\n",
      "INSTANCE 59 - p = 1\n",
      "INSTANCE 60 - p = 1\n",
      "INSTANCE 61 - p = 1\n",
      "INSTANCE 62 - p = 1\n",
      "INSTANCE 63 - p = 1\n",
      "INSTANCE 64 - p = 1\n",
      "INSTANCE 65 - p = 1\n",
      "INSTANCE 66 - p = 1\n",
      "INSTANCE 67 - p = 1\n",
      "INSTANCE 68 - p = 1\n",
      "INSTANCE 69 - p = 1\n",
      "INSTANCE 70 - p = 1\n",
      "INSTANCE 71 - p = 1\n",
      "INSTANCE 72 - p = 1\n",
      "INSTANCE 73 - p = 1\n",
      "INSTANCE 74 - p = 1\n",
      "INSTANCE 75 - p = 1\n",
      "INSTANCE 76 - p = 1\n",
      "INSTANCE 77 - p = 1\n",
      "INSTANCE 78 - p = 1\n",
      "INSTANCE 79 - p = 1\n",
      "INSTANCE 80 - p = 1\n",
      "INSTANCE 81 - p = 1\n",
      "INSTANCE 82 - p = 1\n",
      "INSTANCE 83 - p = 1\n",
      "INSTANCE 84 - p = 1\n",
      "INSTANCE 85 - p = 1\n",
      "INSTANCE 86 - p = 1\n",
      "INSTANCE 87 - p = 1\n",
      "INSTANCE 88 - p = 1\n",
      "INSTANCE 89 - p = 1\n",
      "INSTANCE 90 - p = 1\n",
      "INSTANCE 91 - p = 1\n",
      "INSTANCE 92 - p = 1\n",
      "INSTANCE 93 - p = 1\n",
      "INSTANCE 94 - p = 1\n",
      "INSTANCE 95 - p = 1\n",
      "INSTANCE 96 - p = 1\n",
      "INSTANCE 97 - p = 1\n",
      "INSTANCE 98 - p = 1\n",
      "INSTANCE 99 - p = 1\n",
      "INSTANCE 100 - p = 1\n",
      "INSTANCE 1 - p = 2\n",
      "INSTANCE 2 - p = 2\n",
      "INSTANCE 3 - p = 2\n",
      "INSTANCE 4 - p = 2\n",
      "INSTANCE 5 - p = 2\n",
      "INSTANCE 6 - p = 2\n",
      "INSTANCE 7 - p = 2\n",
      "INSTANCE 8 - p = 2\n",
      "INSTANCE 9 - p = 2\n",
      "INSTANCE 10 - p = 2\n",
      "INSTANCE 11 - p = 2\n",
      "INSTANCE 12 - p = 2\n",
      "INSTANCE 13 - p = 2\n",
      "INSTANCE 14 - p = 2\n",
      "INSTANCE 15 - p = 2\n",
      "INSTANCE 16 - p = 2\n",
      "INSTANCE 17 - p = 2\n",
      "INSTANCE 18 - p = 2\n",
      "INSTANCE 19 - p = 2\n",
      "INSTANCE 20 - p = 2\n",
      "INSTANCE 21 - p = 2\n",
      "INSTANCE 22 - p = 2\n",
      "INSTANCE 23 - p = 2\n",
      "INSTANCE 24 - p = 2\n",
      "INSTANCE 25 - p = 2\n",
      "INSTANCE 26 - p = 2\n",
      "INSTANCE 27 - p = 2\n",
      "INSTANCE 28 - p = 2\n",
      "INSTANCE 29 - p = 2\n",
      "INSTANCE 30 - p = 2\n",
      "INSTANCE 31 - p = 2\n",
      "INSTANCE 32 - p = 2\n",
      "INSTANCE 33 - p = 2\n",
      "INSTANCE 34 - p = 2\n",
      "INSTANCE 35 - p = 2\n",
      "INSTANCE 36 - p = 2\n",
      "INSTANCE 37 - p = 2\n",
      "INSTANCE 38 - p = 2\n",
      "INSTANCE 39 - p = 2\n",
      "INSTANCE 40 - p = 2\n",
      "INSTANCE 41 - p = 2\n",
      "INSTANCE 42 - p = 2\n",
      "INSTANCE 43 - p = 2\n",
      "INSTANCE 44 - p = 2\n",
      "INSTANCE 45 - p = 2\n",
      "INSTANCE 46 - p = 2\n",
      "INSTANCE 47 - p = 2\n",
      "INSTANCE 48 - p = 2\n",
      "INSTANCE 49 - p = 2\n",
      "INSTANCE 50 - p = 2\n",
      "INSTANCE 51 - p = 2\n",
      "INSTANCE 52 - p = 2\n",
      "INSTANCE 53 - p = 2\n",
      "INSTANCE 54 - p = 2\n",
      "INSTANCE 55 - p = 2\n",
      "INSTANCE 56 - p = 2\n",
      "INSTANCE 57 - p = 2\n",
      "INSTANCE 58 - p = 2\n",
      "INSTANCE 59 - p = 2\n",
      "INSTANCE 60 - p = 2\n",
      "INSTANCE 61 - p = 2\n",
      "INSTANCE 62 - p = 2\n",
      "INSTANCE 63 - p = 2\n",
      "INSTANCE 64 - p = 2\n",
      "INSTANCE 65 - p = 2\n",
      "INSTANCE 66 - p = 2\n",
      "INSTANCE 67 - p = 2\n",
      "INSTANCE 68 - p = 2\n",
      "INSTANCE 69 - p = 2\n",
      "INSTANCE 70 - p = 2\n",
      "INSTANCE 71 - p = 2\n",
      "INSTANCE 72 - p = 2\n",
      "INSTANCE 73 - p = 2\n",
      "INSTANCE 74 - p = 2\n",
      "INSTANCE 75 - p = 2\n",
      "INSTANCE 76 - p = 2\n",
      "INSTANCE 77 - p = 2\n",
      "INSTANCE 78 - p = 2\n",
      "INSTANCE 79 - p = 2\n",
      "INSTANCE 80 - p = 2\n",
      "INSTANCE 81 - p = 2\n",
      "INSTANCE 82 - p = 2\n",
      "INSTANCE 83 - p = 2\n",
      "INSTANCE 84 - p = 2\n",
      "INSTANCE 85 - p = 2\n",
      "INSTANCE 86 - p = 2\n",
      "INSTANCE 87 - p = 2\n",
      "INSTANCE 88 - p = 2\n",
      "INSTANCE 89 - p = 2\n",
      "INSTANCE 90 - p = 2\n",
      "INSTANCE 91 - p = 2\n",
      "INSTANCE 92 - p = 2\n",
      "INSTANCE 93 - p = 2\n",
      "INSTANCE 94 - p = 2\n",
      "INSTANCE 95 - p = 2\n",
      "INSTANCE 96 - p = 2\n",
      "INSTANCE 97 - p = 2\n",
      "INSTANCE 98 - p = 2\n",
      "INSTANCE 99 - p = 2\n",
      "INSTANCE 100 - p = 2\n",
      "INSTANCE 1 - p = 5\n",
      "INSTANCE 2 - p = 5\n",
      "INSTANCE 3 - p = 5\n",
      "INSTANCE 4 - p = 5\n",
      "INSTANCE 5 - p = 5\n",
      "INSTANCE 6 - p = 5\n",
      "INSTANCE 7 - p = 5\n",
      "INSTANCE 8 - p = 5\n",
      "INSTANCE 9 - p = 5\n",
      "INSTANCE 10 - p = 5\n",
      "INSTANCE 11 - p = 5\n",
      "INSTANCE 12 - p = 5\n",
      "INSTANCE 13 - p = 5\n",
      "INSTANCE 14 - p = 5\n",
      "INSTANCE 15 - p = 5\n",
      "INSTANCE 16 - p = 5\n",
      "INSTANCE 17 - p = 5\n",
      "INSTANCE 18 - p = 5\n",
      "INSTANCE 19 - p = 5\n",
      "INSTANCE 20 - p = 5\n",
      "INSTANCE 21 - p = 5\n",
      "INSTANCE 22 - p = 5\n",
      "INSTANCE 23 - p = 5\n",
      "INSTANCE 24 - p = 5\n",
      "INSTANCE 25 - p = 5\n",
      "INSTANCE 26 - p = 5\n",
      "INSTANCE 27 - p = 5\n",
      "INSTANCE 28 - p = 5\n",
      "INSTANCE 29 - p = 5\n",
      "INSTANCE 30 - p = 5\n",
      "INSTANCE 31 - p = 5\n",
      "INSTANCE 32 - p = 5\n",
      "INSTANCE 33 - p = 5\n",
      "INSTANCE 34 - p = 5\n",
      "INSTANCE 35 - p = 5\n",
      "INSTANCE 36 - p = 5\n",
      "INSTANCE 37 - p = 5\n",
      "INSTANCE 38 - p = 5\n",
      "INSTANCE 39 - p = 5\n",
      "INSTANCE 40 - p = 5\n",
      "INSTANCE 41 - p = 5\n",
      "INSTANCE 42 - p = 5\n",
      "INSTANCE 43 - p = 5\n",
      "INSTANCE 44 - p = 5\n",
      "INSTANCE 45 - p = 5\n",
      "INSTANCE 46 - p = 5\n",
      "INSTANCE 47 - p = 5\n",
      "INSTANCE 48 - p = 5\n",
      "INSTANCE 49 - p = 5\n",
      "INSTANCE 50 - p = 5\n",
      "INSTANCE 51 - p = 5\n",
      "INSTANCE 52 - p = 5\n",
      "INSTANCE 53 - p = 5\n",
      "INSTANCE 54 - p = 5\n",
      "INSTANCE 55 - p = 5\n",
      "INSTANCE 56 - p = 5\n",
      "INSTANCE 57 - p = 5\n",
      "INSTANCE 58 - p = 5\n",
      "INSTANCE 59 - p = 5\n",
      "INSTANCE 60 - p = 5\n",
      "INSTANCE 61 - p = 5\n",
      "INSTANCE 62 - p = 5\n",
      "INSTANCE 63 - p = 5\n",
      "INSTANCE 64 - p = 5\n",
      "INSTANCE 65 - p = 5\n",
      "INSTANCE 66 - p = 5\n",
      "INSTANCE 67 - p = 5\n",
      "INSTANCE 68 - p = 5\n",
      "INSTANCE 69 - p = 5\n",
      "INSTANCE 70 - p = 5\n",
      "INSTANCE 71 - p = 5\n",
      "INSTANCE 72 - p = 5\n",
      "INSTANCE 73 - p = 5\n",
      "INSTANCE 74 - p = 5\n",
      "INSTANCE 75 - p = 5\n",
      "INSTANCE 76 - p = 5\n",
      "INSTANCE 77 - p = 5\n",
      "INSTANCE 78 - p = 5\n",
      "INSTANCE 79 - p = 5\n",
      "INSTANCE 80 - p = 5\n",
      "INSTANCE 81 - p = 5\n",
      "INSTANCE 82 - p = 5\n",
      "INSTANCE 83 - p = 5\n",
      "INSTANCE 84 - p = 5\n",
      "INSTANCE 85 - p = 5\n",
      "INSTANCE 86 - p = 5\n",
      "INSTANCE 87 - p = 5\n",
      "INSTANCE 88 - p = 5\n",
      "INSTANCE 89 - p = 5\n",
      "INSTANCE 90 - p = 5\n",
      "INSTANCE 91 - p = 5\n",
      "INSTANCE 92 - p = 5\n",
      "INSTANCE 93 - p = 5\n",
      "INSTANCE 94 - p = 5\n",
      "INSTANCE 95 - p = 5\n",
      "INSTANCE 96 - p = 5\n",
      "INSTANCE 97 - p = 5\n",
      "INSTANCE 98 - p = 5\n",
      "INSTANCE 99 - p = 5\n",
      "INSTANCE 100 - p = 5\n",
      "INSTANCE 1 - p = 10\n",
      "INSTANCE 2 - p = 10\n",
      "INSTANCE 3 - p = 10\n",
      "INSTANCE 4 - p = 10\n",
      "INSTANCE 5 - p = 10\n",
      "INSTANCE 6 - p = 10\n",
      "INSTANCE 7 - p = 10\n",
      "INSTANCE 8 - p = 10\n",
      "INSTANCE 9 - p = 10\n",
      "INSTANCE 10 - p = 10\n",
      "INSTANCE 11 - p = 10\n",
      "INSTANCE 12 - p = 10\n",
      "INSTANCE 13 - p = 10\n",
      "INSTANCE 14 - p = 10\n",
      "INSTANCE 15 - p = 10\n",
      "INSTANCE 16 - p = 10\n",
      "INSTANCE 17 - p = 10\n",
      "INSTANCE 18 - p = 10\n",
      "INSTANCE 19 - p = 10\n",
      "INSTANCE 20 - p = 10\n",
      "INSTANCE 21 - p = 10\n",
      "INSTANCE 22 - p = 10\n",
      "INSTANCE 23 - p = 10\n",
      "INSTANCE 24 - p = 10\n",
      "INSTANCE 25 - p = 10\n",
      "INSTANCE 26 - p = 10\n",
      "INSTANCE 27 - p = 10\n",
      "INSTANCE 28 - p = 10\n",
      "INSTANCE 29 - p = 10\n",
      "INSTANCE 30 - p = 10\n",
      "INSTANCE 31 - p = 10\n",
      "INSTANCE 32 - p = 10\n",
      "INSTANCE 33 - p = 10\n",
      "INSTANCE 34 - p = 10\n",
      "INSTANCE 35 - p = 10\n",
      "INSTANCE 36 - p = 10\n",
      "INSTANCE 37 - p = 10\n",
      "INSTANCE 38 - p = 10\n",
      "INSTANCE 39 - p = 10\n",
      "INSTANCE 40 - p = 10\n",
      "INSTANCE 41 - p = 10\n",
      "INSTANCE 42 - p = 10\n",
      "INSTANCE 43 - p = 10\n",
      "INSTANCE 44 - p = 10\n",
      "INSTANCE 45 - p = 10\n",
      "INSTANCE 46 - p = 10\n",
      "INSTANCE 47 - p = 10\n",
      "INSTANCE 48 - p = 10\n",
      "INSTANCE 49 - p = 10\n",
      "INSTANCE 50 - p = 10\n",
      "INSTANCE 51 - p = 10\n",
      "INSTANCE 52 - p = 10\n",
      "INSTANCE 53 - p = 10\n",
      "INSTANCE 54 - p = 10\n",
      "INSTANCE 55 - p = 10\n",
      "INSTANCE 56 - p = 10\n",
      "INSTANCE 57 - p = 10\n",
      "INSTANCE 58 - p = 10\n",
      "INSTANCE 59 - p = 10\n",
      "INSTANCE 60 - p = 10\n",
      "INSTANCE 61 - p = 10\n",
      "INSTANCE 62 - p = 10\n",
      "INSTANCE 63 - p = 10\n",
      "INSTANCE 64 - p = 10\n",
      "INSTANCE 65 - p = 10\n",
      "INSTANCE 66 - p = 10\n",
      "INSTANCE 67 - p = 10\n",
      "INSTANCE 68 - p = 10\n",
      "INSTANCE 69 - p = 10\n",
      "INSTANCE 70 - p = 10\n",
      "INSTANCE 71 - p = 10\n",
      "INSTANCE 72 - p = 10\n",
      "INSTANCE 73 - p = 10\n",
      "INSTANCE 74 - p = 10\n",
      "INSTANCE 75 - p = 10\n",
      "INSTANCE 76 - p = 10\n",
      "INSTANCE 77 - p = 10\n",
      "INSTANCE 78 - p = 10\n",
      "INSTANCE 79 - p = 10\n",
      "INSTANCE 80 - p = 10\n",
      "INSTANCE 81 - p = 10\n",
      "INSTANCE 82 - p = 10\n",
      "INSTANCE 83 - p = 10\n",
      "INSTANCE 84 - p = 10\n",
      "INSTANCE 85 - p = 10\n",
      "INSTANCE 86 - p = 10\n",
      "INSTANCE 87 - p = 10\n",
      "INSTANCE 88 - p = 10\n",
      "INSTANCE 89 - p = 10\n",
      "INSTANCE 90 - p = 10\n",
      "INSTANCE 91 - p = 10\n",
      "INSTANCE 92 - p = 10\n",
      "INSTANCE 93 - p = 10\n",
      "INSTANCE 94 - p = 10\n",
      "INSTANCE 95 - p = 10\n",
      "INSTANCE 96 - p = 10\n",
      "INSTANCE 97 - p = 10\n",
      "INSTANCE 98 - p = 10\n",
      "INSTANCE 99 - p = 10\n",
      "INSTANCE 100 - p = 10\n",
      "{1: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 2: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0009999275207519531}, 3: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 4: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0010001659393310547}, 5: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0010001659393310547}, 6: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 7: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 8: {1: 0.0, 2: 0.0, 5: 0.002000093460083008, 10: 0.0}, 9: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 10: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 11: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 12: {1: 0.0009999275207519531, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 13: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 14: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 15: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 16: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 17: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 18: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 19: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 20: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 21: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 22: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 23: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 24: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 25: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 26: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 27: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.002000093460083008}, 28: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 29: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 30: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0010001659393310547, 10: 0.0}, 31: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0010001659393310547}, 32: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 33: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 34: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 35: {1: 0.002000093460083008, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 36: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 37: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 38: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 39: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 40: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 41: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 42: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 43: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 44: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 45: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 46: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 47: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 48: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 49: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 50: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0019998550415039062}, 51: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 52: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 53: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 54: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 55: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 56: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 57: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0009999275207519531}, 58: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 59: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 60: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 61: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 62: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 63: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 64: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 65: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0}, 66: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 67: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 68: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 69: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 70: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0}, 71: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0009999275207519531, 10: 0.0}, 72: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 73: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 74: {1: 0.0, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0010001659393310547}, 75: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 76: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 77: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 78: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 79: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 80: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 81: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 82: {1: 0.0, 2: 0.0010001659393310547, 5: 0.0, 10: 0.0009999275207519531}, 83: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 84: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 85: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 86: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 87: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 88: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 89: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 90: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0009999275207519531}, 91: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0}, 92: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 93: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 94: {1: 0.0, 2: 0.0009999275207519531, 5: 0.0, 10: 0.0}, 95: {1: 0.0, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 96: {1: 0.0, 2: 0.0, 5: 0.0, 10: 0.0009999275207519531}, 97: {1: 0.0009999275207519531, 2: 0.0, 5: 0.0, 10: 0.0}, 98: {1: 0.002000093460083008, 2: 0.0, 5: 0.0009999275207519531, 10: 0.0}, 99: {1: 0.0010001659393310547, 2: 0.0, 5: 0.0, 10: 0.0}, 100: {1: 0.0, 2: 0.0, 5: 0.0010001659393310547, 10: 0.0009999275207519531}}\n",
      "{1: {1: 0.4067155869191848, 2: -inf, 5: 0.9271628213907981, 10: 0.96113051380953}, 2: {1: 1.4768626519718437, 2: 1.643307157533968, 5: 1.7431738608712426, 10: 1.7764627619836675}, 3: {1: 1.2225245488966103, 2: 1.3690438584323916, 5: 1.4569554441538606, 10: 1.4862593060610167}, 4: {1: 0.660278623989512, 2: 0.7853312431066845, 5: 0.8603628145769882, 10: 0.8853733384004226}, 5: {1: -inf, 2: 1.3463713707717164, 5: 1.460700423667522, 10: 1.498810107966124}, 6: {1: 1.1347335062187704, 2: 1.5126612714178216, 5: 1.626629124791557, 10: 1.6646184092494687}, 7: {1: 1.2633027100185215, 2: 1.4174844675773106, 5: 1.5099935221125842, 10: 1.5408298736243418}, 8: {1: 0.9911106934711577, 2: 1.151507848500097, 5: 1.265019570743355, 10: 1.304759755523139}, 9: {1: 0.5456538486459475, 2: 0.7011817595587606, 5: 0.8012114218352866, 10: 0.8364612531964136}, 10: {1: 1.1315361771559571, 2: 1.284061087082043, 5: 1.3755760330376945, 10: 1.4060810150229117}, 11: {1: -inf, 2: 1.2016663894390975, 5: 1.322627406043869, 10: 1.362947744912126}, 12: {1: 1.0109536636428749, 2: 1.1641732121846171, 5: 1.2561049413096625, 10: 1.286748851018011}, 13: {1: 0.5253829487183219, 2: 0.601166271021288, 5: 0.6483065464527078, 10: 0.6656588728988341}, 14: {1: -inf, 2: 0.9466194086874584, 5: 1.0431055093704744, 10: 1.07526754293148}, 15: {1: 0.725845733872731, 2: 1.0812654061239972, 5: 1.1906413541189047, 10: 1.2271000034505404}, 16: {1: -inf, 2: 1.3202846527266, 5: 1.4205335055594461, 10: 1.4539497898370617}, 17: {1: -inf, 2: 1.1798270058506626, 5: 1.2734742329770559, 10: 1.30468997535252}, 18: {1: 0.4325767476163096, 2: 0.9107152372777133, 5: 1.0086643280249612, 10: 1.0413140249407105}, 19: {1: -inf, 2: 1.696522948593198, 5: 1.8093364532655802, 10: 1.8469409548230407}, 20: {1: 1.1256497995202897, 2: 1.668020198398486, 5: 1.7868459932142255, 10: 1.8264545914861385}, 21: {1: -inf, 2: 0.7304736246945129, 5: 0.8417579537809416, 10: 0.8790215337775742}, 22: {1: 0.9329594283391921, 2: 1.063540487069123, 5: 1.1418891223070815, 10: 1.1680053340530676}, 23: {1: 0.8383715678662268, 2: -inf, 5: 1.3456562778571453, 10: 1.3865538090972902}, 24: {1: -inf, 2: -inf, 5: 1.029722590396219, 10: 1.0611574056555542}, 25: {1: -inf, 2: 1.1390104186223136, 5: 1.224210655947521, 10: 1.2526107350559235}, 26: {1: -inf, 2: -inf, 5: 1.165110492167985, 10: 1.2008861647448565}, 27: {1: 0.34914031225357567, 2: 0.6843455769412163, 5: 0.7576826916432221, 10: 0.782204997466057}, 28: {1: 0.3960639788265869, 2: 0.45428117871229295, 5: 0.5980173732904112, 10: 0.6242309127110912}, 29: {1: 0.8789394053008732, 2: 1.0425766971506603, 5: 1.1655360264187586, 10: 1.207421348162948}, 30: {1: 0.9906428851052168, 2: 1.1052953641553482, 5: 1.1740868515854272, 10: 1.1970173473954535}, 31: {1: 1.179106719014978, 2: 1.3554150426260918, 5: 1.4689222460758815, 10: 1.5067579805591447}, 32: {1: 1.23473907600653, 2: 1.3865537319116326, 5: 1.477642525454694, 10: 1.5080054566357144}, 33: {1: -inf, 2: 1.1519344944445393, 5: 1.259824703072228, 10: 1.295788105948124}, 34: {1: 0.6322375320045597, 2: 0.7428618519525263, 5: 0.8092364439213061, 10: 0.8313613079108995}, 35: {1: 1.3935690703407255, 2: 1.5510349307190334, 5: 1.6487410132466547, 10: 1.6846463244139185}, 36: {1: 1.337142978059032, 2: 1.4928831181155118, 5: 1.5863272021494, 10: 1.6174752301606956}, 37: {1: 1.4942445572116136, 2: 1.668622427724071, 5: 1.7732491500315457, 10: 1.8081247241340372}, 38: {1: 0.6494691816644834, 2: 1.0059852534340563, 5: 1.1200944958161452, 10: 1.158130909943508}, 39: {1: 1.3257431256330383, 2: 1.4902201560678194, 5: 1.5894720326644092, 10: 1.6273761766407646}, 40: {1: 0.7021648502179288, 2: 0.8402449455703923, 5: 0.9230930027818705, 10: 0.9558322783905214}, 41: {1: -inf, 2: 0.9040944156984454, 5: 1.0166960166401149, 10: 1.0557580627488603}, 42: {1: 0.5119136710688611, 2: 0.5758600464814861, 5: 0.6142278717290612, 10: 0.6270171468115862}, 43: {1: -inf, 2: -inf, 5: 1.333907940261227, 10: 1.3728635964766984}, 44: {1: 0.5343867517768517, 2: 0.7940887929256566, 5: 0.8770015320648263, 10: 0.9046391117778829}, 45: {1: 0.36143324912778074, 2: 0.7780880723220314, 5: 0.9759636732674217, 10: 1.0127581693187366}, 46: {1: -inf, 2: 1.1557909450664465, 5: 1.2622543891119196, 10: 1.2977422037937438}, 47: {1: -inf, 2: -inf, 5: 0.8643264882837406, 10: 0.89749295393456}, 48: {1: -inf, 2: 1.4939556887636736, 5: 1.593215574524552, 10: 1.6263022031115115}, 49: {1: 0.26679301951862444, 2: 0.5616980671816139, 5: 0.6565681668929724, 10: 0.6881915334634252}, 50: {1: 1.0259967011005682, 2: 1.2832250069385664, 5: 1.4907055792522832, 10: 1.5291420778368225}, 51: {1: 0.7388722216387206, 2: 0.8632991982649737, 5: 0.9379553842407257, 10: 0.9628407795659765}, 52: {1: 0.40924897282742145, 2: 0.8610342411551385, 5: 0.9535315636410115, 10: 0.9843640044696356}, 53: {1: -inf, 2: -inf, 5: 1.0227573409753128, 10: 1.0588468968422462}, 54: {1: -inf, 2: 1.0517357086824963, 5: 1.1605461575426141, 10: 1.1968163071626532}, 55: {1: -inf, 2: 1.1561934797192446, 5: 1.270881195475288, 10: 1.3091104340606359}, 56: {1: -inf, 2: 1.3974140314043701, 5: 1.5053369565446184, 10: 1.5413112649247014}, 57: {1: -inf, 2: 1.3601487558647594, 5: 1.4567133954736389, 10: 1.4889016086765987}, 58: {1: 0.22182660812049726, 2: 0.5664359725925162, 5: 0.7716121530005213, 10: 0.8109240322510506}, 59: {1: -inf, 2: 1.394324094214611, 5: 1.4917890314413969, 10: 1.525525224329769}, 60: {1: 1.479079755768991, 2: 1.6562147663794886, 5: 1.7624957727457875, 10: 1.7979227748678872}, 61: {1: 0.6289677543827649, 2: 0.7657386996184805, 5: 0.8530629256546701, 10: 0.8821710010000667}, 62: {1: -inf, 2: 0.922008018251073, 5: 1.0299303109902844, 10: 1.0659044085700216}, 63: {1: 0.7177255208755906, 2: 1.0715964075387738, 5: 1.1820589043649448, 10: 1.218879736640335}, 64: {1: 0.7099147783606037, 2: 0.8593307167588069, 5: 0.9489802797977287, 10: 0.9788634674773694}, 65: {1: 0.7566260869817387, 2: 0.879424569721681, 5: 0.9531036593656463, 10: 0.9776633559136347}, 66: {1: -inf, 2: -inf, 5: 1.1660668628519764, 10: 1.205410774616663}, 67: {1: 1.2488778015539361, 2: 1.3935982647475746, 5: 1.4804305426637576, 10: 1.5093746353024857}, 68: {1: 0.7805575209632734, 2: 0.9026912253288955, 5: 0.9759714479482688, 10: 1.000398188821393}, 69: {1: -inf, 2: 0.9135825116328598, 5: 1.2044363648250898, 10: 1.2423202549940586}, 70: {1: -inf, 2: -inf, 5: 1.373190814231466, 10: 1.414061945904367}, 71: {1: -inf, 2: -inf, 5: 1.067314078947868, 10: 1.107072674555167}, 72: {1: -inf, 2: -inf, 5: 1.2390338367974585, 10: 1.2798976507042077}, 73: {1: -inf, 2: 0.44251713749853927, 5: 0.5470688293939594, 10: 0.5666337958789189}, 74: {1: 0.29992985631521507, 2: -inf, 5: 0.8628850831374038, 10: 0.9021271532838739}, 75: {1: 0.8544093715826714, 2: 0.9967508811724236, 5: 1.082155786926275, 10: 1.1106240888442256}, 76: {1: 1.3983020110112123, 2: 1.5643265556962684, 5: 1.6639412825073019, 10: 1.6971461914443133}, 77: {1: 0.7521059468002065, 2: 0.8336255362811328, 5: 1.0290238443241861, 10: 1.0661545549518063}, 78: {1: 1.0860254228572592, 2: 1.2627888442257067, 5: 1.3713719215161249, 10: 1.407566280612931}, 79: {1: 0.6482964818402024, 2: 0.8034785475313957, 5: 0.8975022654298268, 10: 0.928843504729304}, 80: {1: -inf, 2: 0.908395896842473, 5: 0.9823500626902006, 10: 1.00700145130611}, 81: {1: 0.28089438237423336, 2: 0.5556815991117824, 5: 0.6223098007309738, 10: 0.6445192012707044}, 82: {1: -inf, 2: 1.3814882980026013, 5: 1.5009118314994367, 10: 1.5407196759983819}, 83: {1: -inf, 2: -inf, 5: 1.4176154652159143, 10: 1.453016714786577}, 84: {1: -inf, 2: 0.6791989282371726, 5: 0.7637873322282418, 10: 0.7919834668919314}, 85: {1: -inf, 2: 1.0536837966075177, 5: 1.157246689139069, 10: 1.1917676533162527}, 86: {1: -inf, 2: -inf, 5: 1.4133603653304072, 10: 1.4480425348848909}, 87: {1: -inf, 2: -inf, 5: 1.059293614355578, 10: 1.0953323344864292}, 88: {1: 0.5539314475226995, 2: 0.6826074961820701, 5: 0.7636841839843402, 10: 0.790709746585097}, 89: {1: -inf, 2: 0.9537434457802161, 5: 1.068228135709064, 10: 1.1063896990186803}, 90: {1: 0.8526084688578477, 2: 1.1878720598276988, 5: 1.2908776332684844, 10: 1.3252128244154129}, 91: {1: -inf, 2: 1.531929688175739, 5: 1.6474042518444412, 10: 1.6858957730673416}, 92: {1: 0.5708341570302538, 2: 0.7439839954023542, 5: 0.852032703300655, 10: 0.8897695725268099}, 93: {1: 0.24697058968426167, 2: 0.5132120965618946, 5: 0.5994597205850017, 10: 0.628208928592704}, 94: {1: -inf, 2: -inf, 5: 1.3525103884302632, 10: 1.386980457178242}, 95: {1: -inf, 2: 1.0537065290780445, 5: 1.3559526189574627, 10: 1.3979822948923417}, 96: {1: -inf, 2: 1.3851283434439716, 5: 1.477330346816648, 10: 1.5080643479408737}, 97: {1: -inf, 2: 1.687604180942344, 5: 1.8040854951119807, 10: 1.8429125998351932}, 98: {1: -inf, 2: 1.3728488290119447, 5: 1.477642309679965, 10: 1.5125734699026383}, 99: {1: -inf, 2: 0.8945858951236627, 5: 0.9798414952954442, 10: 1.008260028686038}, 100: {1: -inf, 2: 1.158850605346264, 5: 1.25656827390163, 10: 1.289140830086752}}\n",
      "{1: {1: 0.4067155869191848, 2: None, 5: 0.9271628213907981, 10: 0.96113051380953}, 2: {1: 1.4768626519718437, 2: 1.643307157533968, 5: 1.7431738608712426, 10: 1.7764627619836675}, 3: {1: 1.2225245488966103, 2: 1.3690438584323916, 5: 1.4569554441538606, 10: 1.4862593060610167}, 4: {1: 0.660278623989512, 2: 0.7853312431066845, 5: 0.8603628145769882, 10: 0.8853733384004226}, 5: {1: None, 2: 1.3463713707717164, 5: 1.460700423667522, 10: 1.498810107966124}, 6: {1: 1.1347335062187704, 2: 1.5126612714178216, 5: 1.626629124791557, 10: 1.6646184092494687}, 7: {1: 1.2633027100185215, 2: 1.4174844675773106, 5: 1.5099935221125842, 10: 1.5408298736243418}, 8: {1: 0.9911106934711577, 2: 1.151507848500097, 5: 1.265019570743355, 10: 1.304759755523139}, 9: {1: 0.5456538486459475, 2: 0.7011817595587606, 5: 0.8012114218352866, 10: 0.8364612531964136}, 10: {1: 1.1315361771559571, 2: 1.284061087082043, 5: 1.3755760330376945, 10: 1.4060810150229117}, 11: {1: None, 2: 1.2016663894390975, 5: 1.322627406043869, 10: 1.362947744912126}, 12: {1: 1.0109536636428749, 2: 1.1641732121846171, 5: 1.2561049413096625, 10: 1.286748851018011}, 13: {1: 0.5253829487183219, 2: 0.601166271021288, 5: 0.6483065464527078, 10: 0.6656588728988341}, 14: {1: None, 2: 0.9466194086874584, 5: 1.0431055093704744, 10: 1.07526754293148}, 15: {1: 0.725845733872731, 2: 1.0812654061239972, 5: 1.1906413541189047, 10: 1.2271000034505404}, 16: {1: None, 2: 1.3202846527266, 5: 1.4205335055594461, 10: 1.4539497898370617}, 17: {1: None, 2: 1.1798270058506626, 5: 1.2734742329770559, 10: 1.30468997535252}, 18: {1: 0.4325767476163096, 2: 0.9107152372777133, 5: 1.0086643280249612, 10: 1.0413140249407105}, 19: {1: None, 2: 1.696522948593198, 5: 1.8093364532655802, 10: 1.8469409548230407}, 20: {1: 1.1256497995202897, 2: 1.668020198398486, 5: 1.7868459932142255, 10: 1.8264545914861385}, 21: {1: None, 2: 0.7304736246945129, 5: 0.8417579537809416, 10: 0.8790215337775742}, 22: {1: 0.9329594283391921, 2: 1.063540487069123, 5: 1.1418891223070815, 10: 1.1680053340530676}, 23: {1: 0.8383715678662268, 2: None, 5: 1.3456562778571453, 10: 1.3865538090972902}, 24: {1: None, 2: None, 5: 1.029722590396219, 10: 1.0611574056555542}, 25: {1: None, 2: 1.1390104186223136, 5: 1.224210655947521, 10: 1.2526107350559235}, 26: {1: None, 2: None, 5: 1.165110492167985, 10: 1.2008861647448565}, 27: {1: 0.34914031225357567, 2: 0.6843455769412163, 5: 0.7576826916432221, 10: 0.782204997466057}, 28: {1: 0.3960639788265869, 2: 0.45428117871229295, 5: 0.5980173732904112, 10: 0.6242309127110912}, 29: {1: 0.8789394053008732, 2: 1.0425766971506603, 5: 1.1655360264187586, 10: 1.207421348162948}, 30: {1: 0.9906428851052168, 2: 1.1052953641553482, 5: 1.1740868515854272, 10: 1.1970173473954535}, 31: {1: 1.179106719014978, 2: 1.3554150426260918, 5: 1.4689222460758815, 10: 1.5067579805591447}, 32: {1: 1.23473907600653, 2: 1.3865537319116326, 5: 1.477642525454694, 10: 1.5080054566357144}, 33: {1: None, 2: 1.1519344944445393, 5: 1.259824703072228, 10: 1.295788105948124}, 34: {1: 0.6322375320045597, 2: 0.7428618519525263, 5: 0.8092364439213061, 10: 0.8313613079108995}, 35: {1: 1.3935690703407255, 2: 1.5510349307190334, 5: 1.6487410132466547, 10: 1.6846463244139185}, 36: {1: 1.337142978059032, 2: 1.4928831181155118, 5: 1.5863272021494, 10: 1.6174752301606956}, 37: {1: 1.4942445572116136, 2: 1.668622427724071, 5: 1.7732491500315457, 10: 1.8081247241340372}, 38: {1: 0.6494691816644834, 2: 1.0059852534340563, 5: 1.1200944958161452, 10: 1.158130909943508}, 39: {1: 1.3257431256330383, 2: 1.4902201560678194, 5: 1.5894720326644092, 10: 1.6273761766407646}, 40: {1: 0.7021648502179288, 2: 0.8402449455703923, 5: 0.9230930027818705, 10: 0.9558322783905214}, 41: {1: None, 2: 0.9040944156984454, 5: 1.0166960166401149, 10: 1.0557580627488603}, 42: {1: 0.5119136710688611, 2: 0.5758600464814861, 5: 0.6142278717290612, 10: 0.6270171468115862}, 43: {1: None, 2: None, 5: 1.333907940261227, 10: 1.3728635964766984}, 44: {1: 0.5343867517768517, 2: 0.7940887929256566, 5: 0.8770015320648263, 10: 0.9046391117778829}, 45: {1: 0.36143324912778074, 2: 0.7780880723220314, 5: 0.9759636732674217, 10: 1.0127581693187366}, 46: {1: None, 2: 1.1557909450664465, 5: 1.2622543891119196, 10: 1.2977422037937438}, 47: {1: None, 2: None, 5: 0.8643264882837406, 10: 0.89749295393456}, 48: {1: None, 2: 1.4939556887636736, 5: 1.593215574524552, 10: 1.6263022031115115}, 49: {1: 0.26679301951862444, 2: 0.5616980671816139, 5: 0.6565681668929724, 10: 0.6881915334634252}, 50: {1: 1.0259967011005682, 2: 1.2832250069385664, 5: 1.4907055792522832, 10: 1.5291420778368225}, 51: {1: 0.7388722216387206, 2: 0.8632991982649737, 5: 0.9379553842407257, 10: 0.9628407795659765}, 52: {1: 0.40924897282742145, 2: 0.8610342411551385, 5: 0.9535315636410115, 10: 0.9843640044696356}, 53: {1: None, 2: None, 5: 1.0227573409753128, 10: 1.0588468968422462}, 54: {1: None, 2: 1.0517357086824963, 5: 1.1605461575426141, 10: 1.1968163071626532}, 55: {1: None, 2: 1.1561934797192446, 5: 1.270881195475288, 10: 1.3091104340606359}, 56: {1: None, 2: 1.3974140314043701, 5: 1.5053369565446184, 10: 1.5413112649247014}, 57: {1: None, 2: 1.3601487558647594, 5: 1.4567133954736389, 10: 1.4889016086765987}, 58: {1: 0.22182660812049726, 2: 0.5664359725925162, 5: 0.7716121530005213, 10: 0.8109240322510506}, 59: {1: None, 2: 1.394324094214611, 5: 1.4917890314413969, 10: 1.525525224329769}, 60: {1: 1.479079755768991, 2: 1.6562147663794886, 5: 1.7624957727457875, 10: 1.7979227748678872}, 61: {1: 0.6289677543827649, 2: 0.7657386996184805, 5: 0.8530629256546701, 10: 0.8821710010000667}, 62: {1: None, 2: 0.922008018251073, 5: 1.0299303109902844, 10: 1.0659044085700216}, 63: {1: 0.7177255208755906, 2: 1.0715964075387738, 5: 1.1820589043649448, 10: 1.218879736640335}, 64: {1: 0.7099147783606037, 2: 0.8593307167588069, 5: 0.9489802797977287, 10: 0.9788634674773694}, 65: {1: 0.7566260869817387, 2: 0.879424569721681, 5: 0.9531036593656463, 10: 0.9776633559136347}, 66: {1: None, 2: None, 5: 1.1660668628519764, 10: 1.205410774616663}, 67: {1: 1.2488778015539361, 2: 1.3935982647475746, 5: 1.4804305426637576, 10: 1.5093746353024857}, 68: {1: 0.7805575209632734, 2: 0.9026912253288955, 5: 0.9759714479482688, 10: 1.000398188821393}, 69: {1: None, 2: 0.9135825116328598, 5: 1.2044363648250898, 10: 1.2423202549940586}, 70: {1: None, 2: None, 5: 1.373190814231466, 10: 1.414061945904367}, 71: {1: None, 2: None, 5: 1.067314078947868, 10: 1.107072674555167}, 72: {1: None, 2: None, 5: 1.2390338367974585, 10: 1.2798976507042077}, 73: {1: None, 2: 0.44251713749853927, 5: 0.5470688293939594, 10: 0.5666337958789189}, 74: {1: 0.29992985631521507, 2: None, 5: 0.8628850831374038, 10: 0.9021271532838739}, 75: {1: 0.8544093715826714, 2: 0.9967508811724236, 5: 1.082155786926275, 10: 1.1106240888442256}, 76: {1: 1.3983020110112123, 2: 1.5643265556962684, 5: 1.6639412825073019, 10: 1.6971461914443133}, 77: {1: 0.7521059468002065, 2: 0.8336255362811328, 5: 1.0290238443241861, 10: 1.0661545549518063}, 78: {1: 1.0860254228572592, 2: 1.2627888442257067, 5: 1.3713719215161249, 10: 1.407566280612931}, 79: {1: 0.6482964818402024, 2: 0.8034785475313957, 5: 0.8975022654298268, 10: 0.928843504729304}, 80: {1: None, 2: 0.908395896842473, 5: 0.9823500626902006, 10: 1.00700145130611}, 81: {1: 0.28089438237423336, 2: 0.5556815991117824, 5: 0.6223098007309738, 10: 0.6445192012707044}, 82: {1: None, 2: 1.3814882980026013, 5: 1.5009118314994367, 10: 1.5407196759983819}, 83: {1: None, 2: None, 5: 1.4176154652159143, 10: 1.453016714786577}, 84: {1: None, 2: 0.6791989282371726, 5: 0.7637873322282418, 10: 0.7919834668919314}, 85: {1: None, 2: 1.0536837966075177, 5: 1.157246689139069, 10: 1.1917676533162527}, 86: {1: None, 2: None, 5: 1.4133603653304072, 10: 1.4480425348848909}, 87: {1: None, 2: None, 5: 1.059293614355578, 10: 1.0953323344864292}, 88: {1: 0.5539314475226995, 2: 0.6826074961820701, 5: 0.7636841839843402, 10: 0.790709746585097}, 89: {1: None, 2: 0.9537434457802161, 5: 1.068228135709064, 10: 1.1063896990186803}, 90: {1: 0.8526084688578477, 2: 1.1878720598276988, 5: 1.2908776332684844, 10: 1.3252128244154129}, 91: {1: None, 2: 1.531929688175739, 5: 1.6474042518444412, 10: 1.6858957730673416}, 92: {1: 0.5708341570302538, 2: 0.7439839954023542, 5: 0.852032703300655, 10: 0.8897695725268099}, 93: {1: 0.24697058968426167, 2: 0.5132120965618946, 5: 0.5994597205850017, 10: 0.628208928592704}, 94: {1: None, 2: None, 5: 1.3525103884302632, 10: 1.386980457178242}, 95: {1: None, 2: 1.0537065290780445, 5: 1.3559526189574627, 10: 1.3979822948923417}, 96: {1: None, 2: 1.3851283434439716, 5: 1.477330346816648, 10: 1.5080643479408737}, 97: {1: None, 2: 1.687604180942344, 5: 1.8040854951119807, 10: 1.8429125998351932}, 98: {1: None, 2: 1.3728488290119447, 5: 1.477642309679965, 10: 1.5125734699026383}, 99: {1: None, 2: 0.8945858951236627, 5: 0.9798414952954442, 10: 1.008260028686038}, 100: {1: None, 2: 1.158850605346264, 5: 1.25656827390163, 10: 1.289140830086752}}\n",
      "{1: {1: 0.3435710877520305, 2: 0.6554219299232605, 5: 0.8592279548591881, 10: 0.9271633396966517}, 2: {1: 1.1439742757829954, 2: 1.476863286907244, 5: 1.6765965665947131, 10: 1.7431743688195627}, 3: {1: 0.9294864887518569, 2: 1.2225251078234198, 5: 1.3983481674809954, 10: 1.4569558912953082}, 4: {1: 0.41017386279305085, 2: 0.6602791010273962, 5: 0.8103421485604264, 10: 0.8603631962072954}, 5: {1: 0.7747268331772301, 2: 1.1558236761632494, 5: 1.3844816365779522, 10: 1.460701005175156}, 6: {1: 0.9668752262811521, 2: 1.3227155737163598, 5: 1.5506511355462114, 10: 1.626629704462035}, 7: {1: 0.954939783057672, 2: 1.2633032981752506, 5: 1.4483212896144517, 10: 1.5099939926379673}, 8: {1: 0.6757863899039472, 2: 0.9911112949056842, 5: 1.187582778598716, 10: 1.2650201737713518}, 9: {1: 0.272525672029855, 2: 0.5456543695975953, 5: 0.733250691482171, 10: 0.8012119597050278}, 10: {1: 0.826486939140142, 2: 1.131536758992314, 5: 1.3145665345363455, 10: 1.37557649850678}, 11: {1: 0.5968620754646738, 2: 1.0000654641472444, 5: 1.2419873435469004, 10: 1.3226280212834147}, 12: {1: 0.7045151510455826, 2: 1.0109542481290676, 5: 1.1948175894819202, 10: 1.2561054088986168}, 13: {1: 0.37381688229325455, 2: 0.5253835268991867, 5: 0.616323398026573, 10: 0.6483070760036858}, 14: {1: 0.4641895187144842, 2: 0.7858098543245384, 5: 0.9787819330021497, 10: 1.0431060001241603}, 15: {1: 0.5628119902888233, 2: 0.8989728548593674, 5: 1.1177246117704727, 10: 1.1906419104337442}, 16: {1: 0.8190410259274092, 2: 1.1532038687035637, 5: 1.3537014468962483, 10: 1.4205340154514792}, 17: {1: 0.7115914656117325, 2: 1.0237488893663762, 5: 1.2110432245405554, 10: 1.273474709291484}, 18: {1: 0.4244644841844078, 2: 0.7474673754425145, 5: 0.9433654323883008, 10: 1.0086648262197992}, 19: {1: 1.1324561424802337, 2: 1.5085011580548406, 5: 1.734128023949816, 10: 1.809337027064737}, 20: {1: 1.0826084939885625, 2: 1.4699779625129779, 5: 1.7076294010496453, 10: 1.7868465975934715}, 21: {1: 0.20983899526291727, 2: 0.5569291895653194, 5: 0.7672313623847833, 10: 0.8417585223780486}, 22: {1: 0.6717978090065384, 2: 0.9329599264663998, 5: 1.0896570973168753, 10: 1.1418895208088475}, 23: {1: 0.6747637118041419, 2: 1.0184768079944884, 5: 1.2638618394236576, 10: 1.3456569019039475}, 24: {1: 0.4715639491240632, 2: 0.778244667893057, 5: 0.9668534395347639, 10: 1.029723070053434}, 25: {1: 0.7130097736847977, 2: 0.9970105647688221, 5: 1.1674109310815326, 10: 1.2242110892983376}, 26: {1: 0.5428458341725134, 2: 0.8789057939198145, 5: 1.0935596929076832, 10: 1.1651110380614262}, 27: {1: 0.32234775770368523, 2: 0.5621175187026916, 5: 0.7087920278655941, 10: 0.7576834376669312}, 28: {1: 0.19677342857054195, 2: 0.39606435894387054, 5: 0.5455906944359197, 10: 0.5980177732772798}, 29: {1: 0.5559656184110846, 2: 0.878940021325659, 5: 1.0831302113619756, 10: 1.16553665513003}, 30: {1: 0.7613383643694522, 2: 0.9906433224697152, 5: 1.1282265597485719, 10: 1.1740875513686244}, 31: {1: 0.8528659743607387, 2: 1.1791073412710031, 5: 1.3932513544368466, 10: 1.468922823403373}, 32: {1: 0.9311103433232777, 2: 1.2347396551334826, 5: 1.4169171263942153, 10: 1.4776429887562563}, 33: {1: 0.6124841372535696, 2: 0.9721181660125316, 5: 1.1878984460784143, 10: 1.2598252518302067}, 34: {1: 0.4109893141069174, 2: 0.6322379540028504, 5: 0.7649870535407522, 10: 0.8092367815199388}, 35: {1: 1.078637950268697, 2: 1.3935696710253125, 5: 1.5825285833423643, 10: 1.6487415611182237}, 36: {1: 1.0256632920475588, 2: 1.3371435721605183, 5: 1.524031621407997, 10: 1.5863276774305888}, 37: {1: 1.1454894813854846, 2: 1.4942452224103995, 5: 1.7034985339855915, 10: 1.7732496821905743}, 38: {1: 0.4906018816705141, 2: 0.8158039082842665, 5: 1.0440222479510393, 10: 1.120095076205765}, 39: {1: 0.9967896921935551, 2: 1.3257437530631166, 5: 1.5231160640988382, 10: 1.5894726110357464}, 40: {1: 0.42600518624676403, 2: 0.702165376951691, 5: 0.8678613860278949, 10: 0.9230934241688803}, 41: {1: 0.3618514852762476, 2: 0.7233472318899914, 5: 0.9402445419585541, 10: 1.0166966126796368}, 42: {1: 0.38402140811573804, 2: 0.511914158940988, 5: 0.5886497118617127, 10: 0.6142282620267626}, 43: {1: 0.6550034233500975, 2: 1.022263433557633, 5: 1.2559972222464255, 10: 1.333908534677368}, 44: {1: 0.41775913600499043, 2: 0.6559014215053727, 5: 0.8217267943547123, 10: 0.8770019537808253}, 45: {1: 0.35175876314679944, 2: 0.692604675907572, 5: 0.9023752426042455, 10: 0.9759642347068755}, 46: {1: 0.6234744017154299, 2: 0.978352548533673, 5: 1.1912793012493492, 10: 1.262254930612998}, 47: {1: 0.26950455957732217, 2: 0.5989953956773147, 5: 0.7979940630622052, 10: 0.8643269943638439}, 48: {1: 0.997656891036639, 2: 1.3285231769062342, 5: 1.5270428222125192, 10: 1.5932160793864383}, 49: {1: 0.15017098835507134, 2: 0.40358183749719934, 5: 0.5933219162863466, 10: 0.6565686494272523}, 50: {1: 0.8687693188076563, 2: 1.1922244719158333, 5: 1.4138331685776289, 10: 1.4907061657467076}, 51: {1: 0.4900187430374611, 2: 0.738872696289968, 5: 0.8881849733112225, 10: 0.9379557639617239}, 52: {1: 0.4007515130566828, 2: 0.7068726250941557, 5: 0.8918671524494737, 10: 0.9535320341067222}, 53: {1: 0.39999197472165565, 2: 0.7340415823934943, 5: 0.9505787799243659, 10: 1.0227578916582332}, 54: {1: 0.5076841561801109, 2: 0.8703856523805031, 5: 1.0880064117410977, 10: 1.1605467109811762}, 55: {1: 0.5827556301038863, 2: 0.9650480159573647, 5: 1.1944233016364798, 10: 1.2708817788071758}, 56: {1: 0.8578000918586071, 2: 1.217543175659435, 5: 1.4333888887088362, 10: 1.5053375054690017}, 57: {1: 0.8773261717618068, 2: 1.199208303791405, 5: 1.3923374602208747, 10: 1.4567138866267944}, 58: {1: 0.14473948512813037, 2: 0.47947401764581765, 5: 0.692988994351136, 10: 0.7716127528521946}, 59: {1: 0.9070000277460628, 2: 1.23188315183535, 5: 1.4268129023558453, 10: 1.491789527173703}, 60: {1: 1.1248104102644356, 2: 1.4790804314854313, 5: 1.691642309074741, 10: 1.76249631331894}, 61: {1: 0.3672658235627632, 2: 0.6289682535405368, 5: 0.7948472191178588, 10: 0.8530633698086518}, 62: {1: 0.3823972407064746, 2: 0.7421382165038457, 5: 0.9579826647519769, 10: 1.029930859911451}, 63: {1: 0.5551462340970498, 2: 0.8874929484634635, 5: 1.108417801655477, 10: 1.1820594662062578}, 64: {1: 0.4110834715407687, 2: 0.7099153483371753, 5: 0.8892143604197048, 10: 0.9489807357789861}, 65: {1: 0.5110295899408906, 2: 0.7566265554207751, 5: 0.9039846410208985, 10: 0.9531040341168754}, 66: {1: 0.48676391196169233, 2: 0.8513163191600485, 5: 1.0873796396630542, 10: 1.166067463192427}, 67: {1: 0.959437427231414, 2: 1.2488783536186912, 5: 1.4225427990381065, 10: 1.480430984315562}, 68: {1: 0.5362905781351373, 2: 0.7805579868663814, 5: 0.9271183389245065, 10: 0.9759718206707552}, 69: {1: 0.5474373836904082, 2: 0.9024878614091909, 5: 1.1286691625494418, 10: 1.2044369428873787}, 70: {1: 0.6599836989216166, 2: 1.0462225404032248, 5: 1.2914491745296397, 10: 1.3731914378754428}, 71: {1: 0.36530100852269276, 2: 0.7492460724245028, 5: 0.9877974944012928, 10: 1.0673146856158913}, 72: {1: 0.5414706884275642, 2: 0.9219010947574523, 5: 1.1585650670572576, 10: 1.2390344603297754}, 73: {1: 0.21333162174315934, 2: 0.39668059388412147, 5: 0.5079394934994335, 10: 0.5470694264693526}, 74: {1: 0.22613566367747076, 2: 0.5608232550645453, 5: 0.7844015416309348, 10: 0.8628856819238746}, 75: {1: 0.5697268953929342, 2: 0.8544099145724388, 5: 1.0252196174821882, 10: 1.082156221318089}, 76: {1: 1.0662535549744767, 2: 1.398302644344589, 5: 1.597531971299981, 10: 1.6639417891740036}, 77: {1: 0.46128601556892884, 2: 0.7521065014962628, 5: 0.9547629896386269, 10: 1.0290244108938673}, 78: {1: 0.7738937628136473, 2: 1.0860260182022898, 5: 1.2989837556046033, 10: 1.3713724737982156}, 79: {1: 0.39254904574561095, 2: 0.6482969696406553, 5: 0.8348202650602321, 10: 0.8975027436591863}, 80: {1: 0.5386255377917587, 2: 0.7851394239508507, 5: 0.9330480377590602, 10: 0.9823508149908786}, 81: {1: 0.23306587922723268, 2: 0.4446350200238276, 5: 0.5778916774286289, 10: 0.62231047850809}, 82: {1: 0.784371389792802, 2: 1.1824498347822536, 5: 1.4212967499210487, 10: 1.5009124389189392}, 83: {1: 0.786380055204424, 2: 1.1344061438758652, 5: 1.3468135062547892, 10: 1.417616005396114}, 84: {1: 0.2562574460804164, 2: 0.5382187927173132, 5: 0.7073954931397337, 10: 0.763787762467113}, 85: {1: 0.5358699923848997, 2: 0.8810796341567373, 5: 1.088205287532812, 10: 1.1572472158871796}, 86: {1: 0.8074641012773895, 2: 1.1359036704044234, 5: 1.343996555429349, 10: 1.4133608945383167}, 87: {1: 0.4207402706653509, 2: 0.770984540692803, 5: 0.987216724001104, 10: 1.0592941642628064}, 88: {1: 0.3190386137454062, 2: 0.5539318955460794, 5: 0.7096338835375449, 10: 0.7636850087390583}, 89: {1: 0.3813207240100327, 2: 0.7629363571061925, 5: 0.991905591389077, 10: 1.0682287180083092}, 90: {1: 0.7012955569265863, 2: 1.0161967589848555, 5: 1.2222077748880666, 10: 1.2908781571819234}, 91: {1: 0.9545576039997332, 2: 1.3394728162287397, 5: 1.5704217967326428, 10: 1.6474048391784444}, 92: {1: 0.23947652711604764, 2: 0.5708347890459816, 5: 0.7800007809326018, 10: 0.8520332528648024}, 93: {1: 0.13539141164903679, 2: 0.3694666048710087, 5: 0.5419617432476976, 10: 0.5994601592631024}, 94: {1: 0.7433703704083792, 2: 1.0767504959108165, 5: 1.2835707769058131, 10: 1.352510914401771}, 95: {1: 0.6518095982138126, 2: 1.0300662121728406, 5: 1.2718939084096639, 10: 1.3559532602794222}, 96: {1: 0.9241189127851392, 2: 1.2314589240273945, 5: 1.4158628135318374, 10: 1.4773308157802885}, 97: {1: 1.1051983506624117, 2: 1.493469397894534, 5: 1.7264318781201575, 10: 1.8040860875665814}, 98: {1: 0.8488820919308502, 2: 1.1981936941575841, 5: 1.4077805222418234, 10: 1.4776428426871702}, 99: {1: 0.4683084363052641, 2: 0.7524937702112019, 5: 0.923004862146663, 10: 0.9798419289278506}, 100: {1: 0.6702628838416378, 2: 0.9959884456928583, 5: 1.1914236585491498, 10: 1.256568770919394}}\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import Model, GRB\n",
    "from math import exp\n",
    "\n",
    "def solve_lagrangian(r, mu, lambda_val):\n",
    "    model = Model(\"Lagrangian\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "    # Variables\n",
    "    y0 = model.addVar(vtype=GRB.CONTINUOUS, name=\"y0\")\n",
    "    y = [model.addVar(vtype=GRB.CONTINUOUS, name=f\"y_{i}\") for i in range(len(mu) - 1)]\n",
    "\n",
    "    # Objective function\n",
    "    model.setObjective(r[0] * y0 + sum((r[i + 1] * exp(mu[i + 1]) - lambda_val) * y[i] for i in range(len(y))), GRB.MAXIMIZE)\n",
    "\n",
    "    # Constraints\n",
    "    model.addConstr(y0 + sum(y[i] for i in range(len(y))) == 1, \"probability_sum\")\n",
    "    for i in range(len(y)):\n",
    "        model.addConstr(y[i] <= y0 * exp(mu[i + 1]), f\"probability_{i}\")\n",
    "\n",
    "    # Optimize\n",
    "    model.optimize()\n",
    "\n",
    "    y0_val = y0.X\n",
    "    y_val = [y[i].X for i in range(len(y))]\n",
    "    return model, y0_val, y_val\n",
    "\n",
    "def is_feasible(y, p):\n",
    "    return sum(1 for yi in y if yi > 0) <= p\n",
    "\n",
    "def find_optimal_lambda_with_heuristic(r, mu, p, epsilon=1e-6):\n",
    "    lambda_low = 0\n",
    "    lambda_high = max(0, (r[1] - r[0]) / p)\n",
    "    best_obj_val = -float('inf')\n",
    "    best_solution = None\n",
    "\n",
    "    primal_bounds = []\n",
    "    dual_bounds = []\n",
    "\n",
    "    while lambda_high - lambda_low > epsilon:\n",
    "        lambda_mid = (lambda_low + lambda_high) / 2\n",
    "        model, y0, y = solve_lagrangian(r, mu, lambda_mid)\n",
    "        obj_val_mid = model.ObjVal\n",
    "\n",
    "        if is_feasible(y, p):\n",
    "            if obj_val_mid > best_obj_val:\n",
    "                best_obj_val = obj_val_mid\n",
    "                best_solution = (y0, y)\n",
    "            primal_bounds.append(best_obj_val)\n",
    "\n",
    "        dual_bounds.append(obj_val_mid)\n",
    "\n",
    "        # We evaluate the function value at lambda_mid + epsilon to determine the direction of the search\n",
    "        if obj_val_mid < (solve_lagrangian(r, mu, lambda_mid + epsilon)[0].objVal):\n",
    "            lambda_high = lambda_mid\n",
    "        else:\n",
    "            lambda_low = lambda_mid\n",
    "\n",
    "    return best_solution, best_obj_val, primal_bounds, dual_bounds, model.Runtime\n",
    "\n",
    "# Small instances\n",
    "r = instances_small_r\n",
    "mu = instances_small_mu\n",
    "n = len(r[0]) -1  # number of products\n",
    "p_values = [1, n // 5, n // 2, n]\n",
    "\n",
    "small_instances_runtimes = {instance: {p: None for p in p_values} for instance in range(1,n_small_instances+1)}\n",
    "small_instances_values = {instance: {p: None for p in p_values} for instance in  range(1,n_small_instances+1)}\n",
    "small_instances_primal_bounds = {instance: {p: None for p in p_values} for instance in  range(1,n_small_instances+1)}\n",
    "small_instances_dual_bounds = {instance: {p: None for p in p_values} for instance in  range(1,n_small_instances+1)}\n",
    "\n",
    "\n",
    "\n",
    "for p in p_values:\n",
    "    for i in range(len(r)):\n",
    "        print(f\"INSTANCE {i+1} - p = {p}\")\n",
    "        best_solution, best_obj_val, primal_bounds, dual_bounds, runtime = find_optimal_lambda_with_heuristic(r[i], mu[i], p)\n",
    "        small_instances_runtimes[i+1][p] = runtime\n",
    "        small_instances_values[i+1][p] = best_obj_val\n",
    "        small_instances_primal_bounds[i+1][p] = primal_bounds[-1] if primal_bounds else None\n",
    "        small_instances_dual_bounds[i+1][p] = dual_bounds[-1] if dual_bounds else None\n",
    "        \n",
    "print(small_instances_runtimes)\n",
    "print(small_instances_values)\n",
    "print(small_instances_primal_bounds)\n",
    "print(small_instances_dual_bounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.3.3 Implement the Lagrangean algorithm and plot a graph of the best bounds obtained so far (primal and dual). Granted that $r_{1}>r_{0}$, prove that solving the lagrangean dual at $\\epsilon$ precision requires the solution of a number of assortment planning problems in\n",
    "\n",
    "$$\n",
    "O\\left(\\log _{2}\\left(\\frac{r_{1}-r_{0}}{p \\epsilon}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Lagrangean Algorithm\n",
    "\n",
    "To implement the Lagrangean algorithm, we follow the binary search approach discussed previously, while keeping track of the best primal and dual bounds. We will also plot the graph of the best bounds obtained so far.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the primal and dual bounds\n",
    "plt.plot(primal_bounds, label='Primal Bound')\n",
    "plt.plot(dual_bounds, label='Dual Bound')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective Value')\n",
    "plt.legend()\n",
    "plt.title('Primal and Dual Bounds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementation and evaluation of models and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the proposed models, as well as the algorithms proposed by you on different instance sizes on the $m=100$ instances provided. The size of the instances is defined by the number of products:\n",
    "\n",
    "- small: $n=10$ products\n",
    "- medium: $n=5.000$ products\n",
    "\n",
    "Each set of instances consists of 2 .csv files:\n",
    "\n",
    "- size-mu.csv: A $(n+1) \\times m$ matrix with $\\mu_{i}$ values\n",
    "- size-r.csv: A $(n+1) \\times m$ matrix with $r_{i}$ values\n",
    "\n",
    "You are also asked to test your models with a large instance of $n=1.000 .000$ products, which you should generate following the same structure as the data you were given. Assume:\n",
    "\n",
    "$$\n",
    "\\mu_{0}=0 \\quad r_{0}=0 \\quad \\mu_{i} \\sim U[0,1] \\quad r_{i} \\sim U[0,1]\n",
    "$$\n",
    "\n",
    "When generating this instance you should either save the values in a .csv file like the one we give you, or set the random number generator seed so as not to produce different instances each time you run your code. For each instance size and each model/algorithm, report the minimum, average and maximum optimal values, as well as the minimum, maximum and average solution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
